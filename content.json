{"pages":[{"title":"Hello, 안녕하세요! 👋","text":"통계학과 컴퓨터 과학을 전공한 6년차 머신러닝 엔지니어입니다. 다양한 도메인에서 확률 예측, 분류, 이상치 탐지, 개인화 추천 등을 위한 머신러닝 모형 개발 및 파이프라인을 담당해왔고, 현재는 자연어 처리(NLP) 영역에서의 연구 및 응용 제품 개발에 큰 관심을 가지고 있습니다. 가장 최근에는 Pre-trained Language Model에 Behavioral Fine-tuning을 적용하여 대화 데이터에 대한 Topic Segmentation 성능을 향상시키는 연구를 수행하였습니다. 보다 상세한 이력 및 최근 프로젝트는 다음 페이지에서 확인하실 수 있습니다.","link":"/about/index.html"}],"posts":[{"title":"Dependency in Language","text":"지난 이야기지난 글에서는 언어 모델을 ‘모든 후보 문자열에 대한 확률 분포’로 정의하였습니다. 그리고 확률을 추정하기 위한 방법으로 MLE Maximum Likelihood Estimation에 대해 살펴보고, 언어의 변동성으로 인한 문제를 해결하기 위해 문장을 단어들의 조합으로 바라보는 관점에 대해 알아보았습니다. 그러나 이러한 접근법에는 문제가 하나 있었죠. 바로 단어들이 나타나는 순서를 고려하지 못한다는 점이었습니다. 이번 글에서는 이러한 문제를 조금 더 일반적인 관점에서 자세히 살펴보고, 이를 완화할 수 있는 방법 중 하나인 N-gram 모델에 대해 알아보도록 하겠습니다. 단어 의존성 Word Dependency문장에서 특정한 단어가 등장할 확률은 함께 등장하는 단어들의 영향을 받습니다. 예를 들어, 특정 문장에 이어서 breakfast라는 단어가 등장할 확률에 대해 생각해보도록 하겠습니다. 누군가가 I ate an English라고 말한 다음에 breakfast라고 말할 확률은, I drunk a Scotch 이후에 breakfast라고 말할 확률보다 높을 것입니다. 반대로, whisky라는 단어가 이어서 등장할 확률은 전자보다는 후자의 문장에서 더 높을 것입니다. 이를 수식으로 표현하자면 다음과 같습니다. P(breakfast | I ate an English) &gt; P(breakfast | I drunk a Scotch)P(whisky | I ate an English) &lt; P(whisky | I drunk a Scotch) 즉, P(breakfast)나 P(whisky)처럼 특정한 단어가 등장할 확률은, 함께 등장하는 단어가 drunk 인지 ate인지, 혹은 English인지 Scotch인지에 따라 달라진다고 할 수 있습니다. 또 다른 예시로, 아래의 세 문장에 대해 한 번 생각해볼까요? (1) 배가 달다.(2) 배가 아프다.(3) 배가 출발하다. 세 문장에서의 배는 각각 과일 pear, 신체 부위 stomach, 그리고 이동수단 ship을 의미하고 있습니다. 그리고 우리는 주변 단어인 ‘달다’, ‘아프다’, ‘출발하다’를 통해서, 각각의 ‘배’가 의미할 수 있는 여러 대상들 중 실제로 어떤 것을 의미하고 있는지를 자연스럽게 추론할 수 있습니다. 종합하자면, 하나의 문장에서 등장하는 각 단어들은 독립적이지 않고 서로에게 영향을 미친다는 사실을 알 수 있으며, 언어가 지니는 이러한 성질을 ‘단어 의존성 Word Dependency’이라고 부릅니다. 단어 순서에 따른 문장 의미 변화우리는 앞선 예시를 통해, 주변에 등장하는 단어에 따라 특정 단어의 발생 확률이 달라진다는 것을 살펴보았습니다. 그렇다면 문장 전체에 등장하는 단어가 정확하게 일치하는 두 문장의 발생 확률은 동일할까요? 지난 글에서 살펴보았던 예제를 다시 한 번 가져와보도록 하겠습니다. (1) I am travelling from Seoul to Edinburgh.(2) I am travelling from Edinburgh to Seoul. 주어진 두 문장에서 사용된 단어는 동일합니다. 하지만 우리는 두 문장에서 from Seoul to Edinburgh와 from Edinburgh to Seoul이 서로 다른 의미를 갖고 있으며, 이로 인해서 전체 문장의 의미 또한 완전히 달라진다는 것을 쉽게 파악할 수 있습니다. 즉, 정확하게 동일한 단어들로 구성된 문장들이라도, 단어의 등장 순서에 따라 전체 문장이 의미하는 바는 달라질 수 있습니다. 두 문장의 의미가 서로 다르다는 것은, 언어 모형이 추정하는 각각의 문장 발생 확률 또한 달라야 한다는 것을 의미합니다. 따라서, 특정 단어의 등장 확률은 주변에 등장하는 단어의 구성 뿐만 아니라 단어의 등장 순서에도 의존적이라고 할 수 있습니다. 단어 순서에 따른 문장의 문법 적합성 변화단어의 순서는 문장의 의미를 결정할 뿐만 아니라, 해당 문장이 문법에 맞게 구성되어 있는지를 결정하기도 합니다. 이를테면, The cat is small.은 정상적인 문장이지만, 두 단어의 순서가 바뀐 The small is cat.은 문법적으로 틀린 문장이 됩니다. 따라서 우리는 언어 모형이 P(The cat is small) 보다 P(The small is cat)를 크게 추정하기를 원할 것입니다. 물론 한국어는 어순 재배치scrambling가 가능한 특성을 지니는 교착어agglutinative language이기 때문에, 고립어Isolating language인 영어보다는 어순의 구성이 조금 더 자유로운 측면이 있습니다. 예를 들어, 어순 재배치를 통해 생성된 아래의 1, 2번 문장들은 서로 유사한 의미를 전달할 수 있는 올바른 문장들입니다. (1) 우리 집 작은 고양이는 귀엽다.(2) 귀엽다. 우리 집 작은 고양이는(3) 고양이는 작은 우리 집 귀엽다. 그러나 한국어 또한 자유어순언어Free word order language는 아니므로 3번과 같이 문법적으로 허용되지 않는 어순이 존재합니다. 따라서 영어와 마찬가지로 단어 순서에 따라 문장의 문법적 수용가능성Linguistic Acceptability이 달라질 수 있습니다. 문장 = 단어들의 시퀀스지난 글에서 우리는 희소 문제를 완화하기 위해 문장을 단어들의 조합으로 해석하고, 문장의 발생 확률을 각각의 단어들의 발생 확률의 곱을 통해 추정하는 방법에 대해 살펴보았습니다. 이러한 관점에 따르면, P(The cat is small)은 P(The)P(cat)P(is)P(small)이고, 곱셈 간에는 교환법칙이 성립하므로 이는 P(The)P(small)P(is)P(cat)과 동일한 값을 지닙니다. 따라서 문법적으로 올바른 문장 ‘The cat is small’과 그렇지 못한 문장 ‘The small is cat’의 발생 확률은 동일한 값으로 추정됩니다. 그러나 앞서 살펴보았던 것처럼 이러한 추정은 바람직하지 않죠. 왜 이러한 문제가 발생했을까요? 우리가 확률 추정의 편의성을 위해 세웠던 ‘문장은 단어의 조합이다’라는 가정이, 언어가 지니고 있는 성질인 ‘단어 의존성’을 반영하지 못하고 있기 때문입니다. 결론부터 이야기하자면, 사실 문장은 단어들의 조합combination이라기보다는 시퀀스sequence에 더 가깝습니다. 이 글에서 계속해서 살펴봤던 것처럼, 문장에서 단어의 순서가 중요한 의미를 지닌다는 의미입니다. 따라서 언어 모형에서 특정 단어의 발생 확률을 추정할 때에도, 해당 단어가 어떤 단어와 함께 어떤 순서로 등장하는지를 반영할 필요가 있습니다. 이를 수식으로 표현하면 다음과 같습니다. $$ P(the\\ cat\\ is\\ small) = P(the)P(cat|the)P(is|the\\ cat)P(small|the\\ cat\\ is) \\ \\neq P(The)P(cat)P(is)P(small)$$ Outro이번 글에서는 단어 사이에 존재하는 의존성에 대해서 살펴보았습니다. 다음 글에서는 이러한 의존성을 언어 모형에 반영하기 위한 방법들에 대해 살펴보도록 하겠습니다.","link":"/Dependency%20in%20Languages/"},{"title":"[Information Theory #1] Entropy에 대한 이해 I","text":"1. 배경요즘에 참여하고 있는 프로젝트에서 참조하는 논문 중 하나가 Multi-dimensional mutual information을 objective function으로 forwarding searching을 수행함으로써 효과적인 변수 탐색을 할 수 있다는 내용을 담고있다. 여기에서 등장하는 Mutual Information은 KL Divergence와 Entropy로부터 꼬리를 물고 이어지는 개념이고, 어렴풋하게만 알고 있던 개념들을 이번 기회에 제대로 정리해볼 겸 이 글을 준비했다. 진행하고 있는 프로젝트와 해당 논문에 대한 내용은 별도의 포스팅으로 다뤄볼 예정이다. 항상 그렇듯이 수식을 통한 엄밀한 증명보다는 수식을 어떻게 바라보아야 직관적으로 이해할 수 있을지, 혹은 그 수식이 도출되기까지의 핵심 아이디어가 무엇인지에 초점을 맞추어서 살펴볼 예정이다. 지금부터 본격적으로 시작해보자. 2. 엔트로피(Entropy)란 무엇인가이 글에서 다룰 엔트로피는 정보이론(Information Theory)의 아버지로 불리는 천재 클로드 섀넌이 창안한 개념이다. 엔트로피라는 용어 자체는 사실 열역학 등의 물리학 분야에서 먼저 사용되었지만, 이 글에서는 섀넌이 정보 이론에서 정립한 정보 엔트로피라는 개념에 대해서 살펴보고자 한다. 두 개의 포스트에 걸쳐서 다음의 두 가지 서로 다른 관점에서 엔트로피를 이해해 볼 예정이다. - 정보량에 대한 측도 Measurement of Information - 불확실성에 대한 측도 Measurement of Uncertainty (of the system) 3. ‘정보량에 대한 측도’라는 관점에서결론부터 말하자면, 엔트로피는 우리가 특정 확률변수의 값을 알게 되었을 때 얻을 수 있는 정보량의 평균을 의미한다고 해석할 수 있다. 조금 덜 통계적인 용어로는 특정 사건의 결과를 알게 되었을 때 얻을 수 있는 정보량의 평균정도로 표현하면 될 것 같다. 우리는 여기에 등장하는 정보량이라는 다소 애매한 개념을 조금 더 명확하게 정의해볼 것이다. 다시 말해서, 다양한 정보들 중에서 도대체 어떤 정보가 더 많은 정보량을 담고 있다고 말할 수 있을지에 대해서 생각해보자. 직관적인 이해를 위해 두 가지 예시를 들어보도록 하겠다. ‘내일 당장 전쟁이 일어난다는 소식’과 ‘내일도 평화로울 것이라는 소식’ 둘 중에 무엇이 우리에게 더 가치있는 정보를 전달한다고 볼 수 있을까? 혹은 ‘내 친구 김갑돌이 어제 로또를 샀는데 1등에 당첨되었다는 소식’과 ‘또 헛탕쳤다는 소식’ 중에서 일반적으로 더 의미있는 정보를 담고 있는 것은 어떤 소식일까? 자연스럽게 생각해보기에, 두 경우 모두 전자가 우리에게 훨씬 더 큰 놀라움과 함께 많은 정보를 전달해준다는 느낌을 받을 수 있을 것이다. 즉, 일반적으로 더 가치있는 정보를 지니고 있는 것은 놀라울만한 소식들이라고 볼 수 있고, 따라서 정보량은 놀라움의 정도에 비례한다고 생각해 볼 수 있다는 것이다. 혹은 이렇게 생각해 볼 수도 있다. 전쟁이 일어나는 것은 전쟁이 일어나지 않는 것보다 훨씬 더 낮은 빈도로 발생하는 사건이다. 로또 당첨 역시 마찬가지다. ~~그래서 내가 당첨이 안되나보다.~~ 다시 말해서, 정보량은 발생 빈도와는 역의 관계에 있다고 할 수 있다. 길게 이야기했지만, 지금까지의 이야기는 사실 다음과 같이 정리할 수 있다. 정보량이 높다= 발생 시 놀라운 사건이 된다= 발생 빈도가 낮다= 발생 확률이 낮다 결국 정보량이라는 다소 모호한 개념을 우리에게 조금 더 친숙한 개념이 확률과 연관지어 생각해 볼 수 있다는 이야기가 된다. 4. 엔트로피의 명확한 정의이번 절에서는 정보량과 확률이 정확히 어떤 관계에 있는지 살펴보고, 이를 이용해서 정보량과 엔트로피(=해당 확률변수가 지닐 수 있는 모든 정보량들의 평균)를 엄밀히 정의해보도록 하자. 편의를 위해 정보량과 확률을 다음과 같은 기호들을 이용하도록 한다. $p(X=x)$: 확률 변수 $X$가 $x$라는 값을 지닐 확률 (Abbr. $p(x)$) $h(X=x)$: 확률 변수 $X$가 $x$라는 값을 지닌다는 사실을 알게됨으로써 우리가 얻을 수 있는 정보량 (Abbr. $h(x)$) 상호 독립인 두 사건에 대해서 생각하는 것으로 이야기를 시작해보자. 이를테면, X: 내일 내가 넘어지는 사건과 Y: 내일 당신이 탈모에 걸리는 사건 정도면 괜찮을 것 같다. 두 사건이 완벽히 독립이라면, 두 사건을 동시에 알게되는 것과 하나씩 차례대로 알게되는 것에는 정보량의 차이가 없을 것이다. 이를 수식으로 표현하면 다음과 같다. $$h(x, y) = h(x) + h(y)$$ 그리고 다시 한 번, 두 사건이 독립(Independent)이라면 정의에 의해 다음이 성립한다. $$p(x, y) = p(x)p(y)$$ 이로부터 우리는 정보량과 확률 사이에는 지수/로그의 관계가 성립한다는 것을 알 수 있다.(이 문장이 와닿지 않는다면 지수 법칙 $c^x \\times c^y = c^{x+y}$를 떠올려보자.) 다시 말해서, 정보량 $h(x)$는 다음과 같이 표현할 수 있다.($\\log$ 앞의 -는 정보량을 항상 양수로 표현해주기 위한 목적이다.) $$h(x) = -\\log p(x)$$ 3절의 가장 첫 부분에서 엔트로피를 ‘특정 확률변수의 값을 알게 되었을 때 얻을 수 있는 정보량의 평균’이라고 정의했었다. 따라서, 엔트로피 $H[X]$는 정보량 $h(X)$의 평균으로 표현할 수 있으며, 이를 수식으로 정리하면 다음과 같다. $H[X]$$\\ \\ = E[h(X)]$$\\ \\ =\\sum_i p(x_i)h(x_i)$$\\ \\ =-\\sum_i p(x_i)\\log p(x_i)$ $X$가 확률적으로 값을 지니는 확률변수이므로, $X$에 대한 함수인 $h(X)$ 또한 확률변수로 볼 수 있고, 따라서 평균값을 구할 수 있다. 바로 그 평균값이 이 글에서 다루고 있는 정보 엔트로피의 정의라고 할 수 있다. 5. 다음 이야기이번 글에서는 정보량이란 무엇인지에 대해 살펴보고, 엔트로피를 정보량에 대한 측도라는 관점에서 이해해보고자 했다. 다음 글에서는 불확실성에 대한 측도라는 관점에서 엔트로피를 살펴볼 예정이다.","link":"/Entropy1/"},{"title":"Hexo 블로그로의 전환과 Vexo 테마 수정기","text":"이 글은 오늘은 또 무슨 딴짓을 하고 살았는지에 대한 기록이다. 하라는 데이터 공부는 안하고 1. 글을 조금 작성해보려하면 찾아오시는 그 분고질병이 다시 찾아와서 블로그 테마를 바꿨다. 사실 테마만 바꾼 수준이 아니라 Static Site Generator 자체를 Jekyll에서 Hexo로 변경했다. 변경한 이유는 여러 가지가 있지만, Jupyter로 생성한 이미지 파일들을 마크다운에서 경로 수정없이 바로 참조할 수 있게 되었다는 점이 가장 좋은 것 같다. 이외에도 포스팅 관련 파일과 블로그 관련 파일을 별도로 버전 관리할 수 있게 되었다는 점도 마음에 든다. Jekyll에서는 Minimal-Mistakes라는 테마를 사용했었다. Minimal-Mistakes는 7.9K Star, 14.5K Fork를 자랑할 뿐만 아니라 Contributor만도 225명이나 되는 굉장히 인기있는 테마다. 이미 어지간한 기능들은 테마에 다 포함되어 있기 때문에, 옵션만 조금 조정해주면 어렵지 않게 커스터마이징해서 쓸 수 있는 강력한 테마였다. 한편 Hexo로 넘어오면서 선택한 테마는 Vexo였다. Hexo의 개발자가 대만 출신이라서 관련 생태계에서 중화권의 비율이 꽤 높은 것으로 알고 있는데, GitHub 프로필을 참고해보면 이 테마 또한 알리페이에서 근무하는 중국 개발자분이 만드신 것 같다. 다른 부분은 별로 고민하지 않았고 단순히 샘플 웹페이지의 디자인이 마음에 들어서 선택했다. 미적 감각이라고는 없는 내가 이런 멋진 테마를 무료로 사용할 수 있다는 것은 정말 감사한 일이지만, 항상 그렇듯이 직접 기획해서 만든 게 아니라면 아쉬운 부분들이 눈에 들어오기 마련이다. 대충 적응해서 써보려고 했지만, 미적인 부분들이 아니라 기능적으로 크리티컬한 부분들이 자꾸 눈에 들어와서 결국 한번 작정하고 수정해보기로 했다. 2. 그래서 어디를 고치겠다고?그래서 일단 수정해야할 부분들을 List-up 해보았다. 개인적으로 수정할 부분들은 아래 리스트 이외에도 조금 더 있었지만, 추가적으로 필요하거나 현재 문제가 있는 기능들은 5개 내외 정도로 정리해 볼 수 있었다. 글의 Table of Contents 링크가 정상적으로 작동하지 않는 문제 Tags 레이아웃에서 각 태그 레이블의 링크가 정상적으로 작동하지 않는 문제 Utterances를 댓글 시스템으로 사용할 수 있도록 추가 블로그 내 검색 기능 추가 Tags 레이아웃 이용 시, 태그 및 포스팅이 정렬되지 않는 문제 기타 자잘한 수정사항들 하드코딩된 한자 제거 깨진 URL/이미지 제거 및 대체 Minor한 CSS 수정 3. 하나씩 차례대로!지금부터는 앞서 언급한 리스트의 각 항목들이 어떤 맥락에서 수정이 필요했고, 또 어떻게 수정을 했는지 하나씩 기록해보도록 하겠다. (1) 글의 Table of Contents 링크가 정상적으로 작동하지 않는 문제정확히 이야기하자면 링크되는 단락에 ‘한글이 포함되어 있는 경우’에만 이동이 정상적으로 이루어지지 않았다. 예를 들자면, Taes' R 시리즈 첫번째 글의 Table of Contents에서, 첫번째 단락인 0. 시작하기에 앞서을 눌러도 아무런 반응이 없었다. 확인해보니 다음과 같이 링크는 정상적으로 생성이 되었는데, 한글에 해당하는 링크의 뒷부분이 정상적으로 디코딩되지 않아서 발생하는 문제였다. 링크 생성 시 URL을 디코딩하도록 수정해줌으로써 간단히 해결할 수 있었다. &lt; 다음: 아래의 두 링크의 내용은 동일하다. &gt; https://taes.me/TaesR01/#0-시작하기에-앞서 https://taes.me/TaesR01/#0-%EC%8B%9C%EC%9E%91%ED%95%98%EA%B8%B0%EC%97%90-%EC%95%9E%EC%84%9C (2) Tags 레이아웃에서 각 태그 레이블의 링크가 정상적으로 작동하지 않는 문제이 문제도 정확히 이야기하자면 태그에 특수문자(공백 포함)가 포함되어 있는 경우에만 발생했다. 예를 들자면 Tag가 TaesR인 경우에는 정상적으로 링크가 작동해서 스크롤링이 되었지만, Taes' R이나 Taes R 처럼 특수문자가 포함된 경우에는 클릭 시에도 아무런 반응이 없었다. 도대체 왜 반응이 없는지 찾느라 시간이 조금 걸리긴 했지만, 정규표현식으로 특수문자들을 Escaping 처리해줌으로써 이 문제도 어렵지 않게 해결할 수 있었다. (3) Utterances를 댓글 시스템으로 사용할 수 있도록 추가Vexo 테마에 Utterances와 유사한 Gitment를 댓글 시스템으로 이용할 수 있는 기능이 포함되어 있긴 했다. 처음에는 그냥 Gitment를 이용하려고 했는데, 내가 설정을 잘못했는지, 아니면 GitHub Pages를 private repository에서 호스팅하고 있어서 그런지 모르겠지만 도저히 정상적으로 작동하지를 않았다. 그래서 결국 Utterances를 삽입하는 스크립트를 추가하고, 테마의 _config.yml 파일에서 선호하는 댓글 시스템을 설정할 수 있도록 수정하였다. (4) 블로그 내 검색 기능 추가사실 이 부분이 가장 어려웠는데… 수정을 하고 있긴 하지만 내가 웹 개발도 Hexo의 구조도 잘 모르는 상태에서 거의 거의 눈치로 때려맞추는 수준으로 고치고 있는 것이 문제였다. hexo-generator-searchdb 모듈을 사용하면 될 것 같긴한데… 사실 이 모듈 설명 페이지에도 대충 search view, search script를 작성한 후에 둘을 connect 해주면 된다고만 되어있고, 나처럼 배경지식이 없는 사람을 위한 친절한 설명까지는 없었다. 자세한 내용은 NexT 테마의 소스 코드를 참고하라고 하길래, 일단 NexT 테마를 들고와서 구조를 뜯어봤다. Local Search와 관련된 소스들과, 해당 소스를 참조하는 소스가 다음과 같은 구조로 되어 있는 것을 확인할 수 있었다. 12345678layout/_layout.swig layout/_partial/header/index.swig layout/_partial/header/menu.swig -- insert search button layout/_partial/search/index.swig layout/_partial/search/localsearch.swig -- search view layout/_third-party/index.swig layout/_third-party/search/localsearch.swig -- connect source/js/local_search.js -- search script 아, 잠깐만… swig요…? vexo는 ejs 쓰던데…? 대충 복사/붙여넣기 해서 돌려볼 생각이었던 나는 확장자가 다른 것을 확인한 순간 내 실력으로 수정하기에는 글러먹었구나 싶었다. 그래도 조금 찾아보니까 둘 다 템플릿 엔진의 한 종류이고, 어떤 식으로 작동하는지는 정도는 확인할 수 있었다. 한번만 해보고 안되면 포기할 생각으로 수정을 시작했다. 일단 참조하는 테마와 유사하게 다음과 같이 구조를 잡았다. 1234567layout/layout.ejs layout/_partial/header.ejs -- insert search button layout/_partial/search/localsearch_button.ejs -- search button layout/_partial/search/localsearch.ejs -- search view layout/_partial/head.ejs layout/_third-party/localsearch.ejs -- connect source/js/local-search.js -- search script 중간에 삽질을 많이 하긴 했지만, 조금만 더 하면 될 것 같아서 대충 눈치로 수정을 거듭하다보니 동작을 했다. 다만, 기존 소스에서는 설정할 수 있는 Hexo의 변수를 자바스크립트(local-search.js)로 넘겨서 처리하는 부분이 있었는데, 어떻게 넘기는건지 잘 모르겠어서 local-search.js에 그냥 하드코딩으로 박아버렸다. (…) 어쨌든 이리저리 수정을 하다보니 정상적으로 검색이 되는 수준까지는 수정을 할 수 있었다. 물론 기능은 작동을 한 후에도 검색 팝업창의 디자인은 조금 이상했지만, 크롬 개발자 도구 통해서 class 속성이나 css를 확인해가면서 적당히 수정해주었다. (View가 모바일로 변경되면서 search 버튼이 사라지는 문제가 있어서 현재는 재수정한 상태이다.) (5) Tags 레이아웃 이용 시, 태그 및 포스팅이 정렬되지 않는 문제이건 사실 버그라기보다는, 원래 기획된 기능과 내가 바라는 기능이 달랐던 부분인 것 같다. Tag를 이용하여 글들을 Grouping할 때에는 굳이 포스팅이나 태그에 대한 정렬이 필요하지 않을 수도 있다. 하지만 나는 Velog에서의 시리즈처럼 글들을 특정 주제로 묶고, 일관된(랜덤하지 않은) 순서로 보여줄 수 있는 레이아웃이 있었으면 했다. 마침 포스팅에 지정할 수 있는 category를 이용한 레이아웃이 없길래, post.categories 변수를 이용해서 시리즈 기능을 할 수 있는 레이아웃을 만들어봤다. Hexo에서 제공하는 변수들과 자바스크립트 자체에 익숙하지 않아서 시행 착오가 조금 있긴 했지만, 그래도 블로그 내 검색 기능 추가하는 것보다는 수월하게 추가할 수 있었다. (6) 기타 자잘한 수정사항들이외에도 이미지나 URL이 깨져있다거나, 한자가 하드코딩 되어있어서 페이지에 보인다거나, CSS 스타일에 조금 조정이 필요하다거나 하는 부분들에 대한 사소한 수정들을 진행했다. 4. PULL REQUEST수정을 해놓고 보니 내게 맞춰서 커스터마이징한 부분들만 있는 것이 아니라, 기능적으로 추가되거나 버그를 수정한 부분들이 꽤 많았다. 그래서, 받아들여질지는 모르겠지만 수정한 내용을 정리를 좀 해서 원본 테마 Repository에 PR를 해보기로 마음먹었다. 이왕 수정한 소스 남들도 같이 사용하면 좋을 것 같고, 솔직히 말하자면 내가 수정에 쏟아부은 하루의 시간이 아깝기도 했다. 그래서 어떤 기능을 추가/수정을 했고, 그러한 작업을 위해서 어느 소스 코드를 수정했는지 아래와 같이 정리하고, README.md에 새로운 기능들을 설정할 수 있는 방법들도 조금 작성해서 Pull Request를 날려보았다. &lt; The List of New/Updated Sources &gt; File Path/Name ChangeType Utterances Local Search Series (Category Layout) Broken Link Hard-coded Chinese Symbols Escaping for Tags/Categories Hyperlink URL Decoding for ToC HiperLink CSS [#] NOTE _config.yml Modified O O O# Removed layout/page.ejs Modified O source/js/local-search.js New O source/css/_partial/search.styl New O layout/_partial/head.ejs Modified O layout/_partial/header.ejs Modified O source/css/style.styl Modified O O O# [modify] display: inline-grid layout/_third-party/localsearch.ejs New O layout/_partial/search/localsearch_button.ejs New O layout/_partial/search/localsearch_view.ejs New O _source/series/ New O layout/series.ejs New O source/css/_partial/categories.styl New O source/js/script.js Modified O O# O Escaping Special symbols in Tag layout/project.ejs Modified O# Replace 开源项目 with GitHub Open Source Projects layout/tags.ejs Modified O# Replace 标签检索 with Tag Retrieval layout/archive.ejs Modified O# Replace 文章归档 with Archives source/css/images/error_icon.png New O# Replaced source/css/_partial/about.styl Modified O# [remove] opacity: 0.25 source/css/_partial/catalog.styl Modified O# [modify] ToC margin/padding source/css/_partial/footer.styl Modified O# [modify] max-width: 960px source/css/_partial/header.styl Modified O# [remove] border-bottom source/css/_partial/markdown.styl Modified O# [modify] overflow-x, font-size 5. PR의 결과는? 수정한 내용을 정리한 정성 덕분이었는지, 소스의 퀄리티에도 불구하고 감사하게도 하루만에 PR이 승인되어 master 브랜치로 병합되었다. 이렇게 뿌듯+1을 추가하며 오늘의 딴짓도 보람차게 마무리했다.","link":"/HexoVexo/"},{"title":"나를 위해 기록하는 Jupyter - HTML&#x2F;PDF 변환 프로세스","text":"노트북(.ipynb) 파일은 물론 Reproducible Research의 개념에 매우 잘 어울리는 포멧이다. 그러나 우리는 블로그 작성을 위해서라거나, Reproducibility에는 전혀 관심이 없을 분들을 위한 최종 보고서 작성을 위해서라거나, 파일 수신자가 노트북 파일을 실행할 수 없는 환경에 있다는 등의 다양한 이유로 노트북 파일을 보다 일반적인 형태의 문서로 변환해야하는 상황과 마주친다. 편리하게도 Jupyter Lab은 다양한 형태로 문서를 내보내는 기능을 제공한다. 노트북 파일을 HTML이나 LaTeX 포멧으로 내보낼수도 있고, 내부적으로 pandoc을 이용하여 pdf로 변환하는 기능도 제공하고 있다. 이러한 과정에서 사용자가 포멧을 직접 설정할 할 수 있는 기능들이 일부 존재하기도 하지만, 아무래도 변환 도구들을 직접 사용하는 것이 아니라 Jupyter를 통해서 접근하는 것이기 때문에 상대적으로 번거롭고 자유도도 떨어진다. 이를 테면 지정된 위치에 custom.css 파일을 작성해놓으면 그 파일을 이용하여 HTML로 내보내준다거나, Pandoc 관련된 옵션을 수동으로 수정할 수 있다거나 하는 식이다. 몇 가지 방법을 시도해봤으나 뭔가 조금씩 마음에 안들었던 나는, 결국 Jupyter Lab에서 마크다운으로 컨텐츠를 내보낸 후, 마크다운을 편집할 수 있는 에디터를 이용하여 내가 원하는 레이아웃으로 최종 변환하는 방법을 선택했다. 이 글에는 나의 삽질의 과정을 기록하고자 한다. 1. Notebook to Markdown노트북 파일을 마크다운으로 변환하는 방법은 매우 간단하다. Jupyter Lab에서 노트북 파일을 오픈한 후에, File &gt; Export Notebook As ... &gt; Export Notebook As Markdown 버튼을 차례로 누르면 Markdown 파일과 실행 셀의 결과로 출력된 이미지 파일들이 모두 포함된 압축 파일의 다운로드가 시작된다. 2. Markdown Editor and Preview Configuration이 글에서는 마크다운 에디터로 Visual Studio Code를 이용할 것이다. Code에는 기본적으로 마크다운 프리뷰 기능이 포함되어 있으며, 프리뷰 렌더링에 사용하는 별도의 css 파일이 존재한다. 해당 파일을 수정하면 Markdown Preview 화면의 출력 결과를 원하는대로 수정할 수 있는데, 여기에는 폰트나 줄간격, (데이터프레임의 출력 결과를 포함하는) 테이블의 표현 방식, 이미지의 정렬 방식 등이 모두 포함된다. Windows 버전 1.45.1 기준으로 렌더링에 사용하는 css 파일의 기본 경로는 다음과 같다. C:\\Users\\[Username]\\AppData\\Local\\Programs\\Microsoft VS Code\\resources\\app\\extensions\\markdown-language-features\\media\\markdown.css 이 파일을 어떤 식으로 수정할 것인지에 대해서는 4. GitHub Style CSS를 참조하자. 3. Extension Configuration다음으로는 마크다운을 HTML과 PDF로 내보내는 기능을 위해 Markdown PDF라는 Extension을 추가로 설치하여 이용할 것이다. 물론 이 Extension에도 Code의 프리뷰 렌더링 css 파일처럼 HTML과 PDF 파일 생성에 이용하는 CSS 파일들이 존재한다. 각각의 CSS 파일의 경로는 다음과 같다. Markdown to HTML C:\\Users\\[Username]\\.vscode\\extensions\\yzane.markdown-pdf-1.4.4\\styles\\markdown.css Markdown to PDF C:\\Users\\[Username]\\.vscode\\extensions\\yzane.markdown-pdf-1.4.4\\styles\\markdown-pdf.css 지금까지 언급한 세 개의 파일을 모두 동일하게 설정할 경우, Code에서 프리뷰로 보는 문서와 동일한 레이아웃의 HTML과 PDF를 외부로 내보낼 수 있다. 4. GitHub Style CSS앞서 언급한 3개의 CSS 파일들을 어떻게 수정할 지는 개인 취향이다. 미적 감각 같은 건 진작에 포기한 나는 GitHub Style Markdown CSS를 가져다가 취향에 맞게 조금만 수정해서 쓰기로 했다. 잠깐 검색을 해보니 두 개 정도를 찾을 수 있었는데, 나는 그 중 하나를 기반으로 다음과 같은 사항들을 추가적으로 수정하여 이용하고 있다. 개인적으로 수정한 css 파일은 이 페이지에서 확인 및 다운받을 수 있다. 기억나는 수정 사항 한글 폰트 추가 설정 코드 영역 폰트 기본 색상 설정 테이블 내부 폰트 사이즈 조정 테이블 전체적인 레이아웃 조정 H1 ~ H6 BOLD 해제 H1 border-bottom: 1px solid #333; table border-collapse: collapse; 내가 수정하기 이전의 원본 css파일은 여기와 저기에서 확인할 수 있다. 5. With VS Code Setting3, 4번 항목에서 각 CSS 파일의 위치를 명시하고 해당 파일을 덮어쓰는 방식으로 진행했지만, 사실 Code와 Extension은 설정 화면에서 custom css 파일을 지정할 수 있는 옵션을 제공하고 있다. 내가 설정을 제대로 못한 것인지, 아니면 Code에 버그가 있는 것인지 상대경로가 아닌 절대경로로 입력하면 제대로 설정이 안되는 상태이긴 하지만… 그래도 참고용으로 설정 화면에서 CSS 파일을 지정할 수 있는 Setting ID를 기록해 놓는다. Setting ID 및 Json (설정 화면에서 ‘markdown styles’로 검색 시 두 옵션 모두 조회 가능) “markdown.styles”: [“TaesHahn.css”] “markdown-pdf.styles”: [“TaesHahn.css”] 추가로, 설정 화면에서 Markdown-pdf: Include Default Styles 옵션은 체크 해제하는 것을 권한다. 아래 경로에 있는 파일이 우리가 직접 지정한 CSS 파일과 중복 적용되는 것을 방지하기 위한 설정이다. Setting ID 및 Json “markdown-pdf.includeDefaultStyles”: false C:\\Users\\[Username]\\.vscode\\extensions\\yzane.markdown-pdf-1.4.4\\node_modules\\jsdom\\lib\\jsdom\\browser\\default-stylesheet.js # 미사용 설정 At the end …이것으로 모든 설정을 마무리하였다. 프로그래밍을 하거나 분석 코드를 작성하는 것만큼, 작성한 글이 읽는 이에게 어떤 형태로 전달되는 지는 매우 중요하다. 이 글이 누군가에게는 도움이 되길 바라면서 글을 마친다. Happy Coding! Happy Editing! ReferencesVisual Studio Code 에서 깃헙 스타일 마크다운 사용하기","link":"/JupToDoc/"},{"title":"Knowledge Integration in Language Models","text":"들어가며 Intro최근 OpenAI의 ChatGPT로부터 시작된 언어모형에 대한 관심이 뜨겁습니다. 마치 알파고와 이세돌 9단 사이의 바둑 대국 이후에 일어났던 인공지능 분야에 대한 관심이 재현된 것만 같네요. ChatGPT가 이러한 뜨거운 관심을 받고 있는 이유는, 마치 사람과도 같은 자연스러운 문장을 생성할 수 있을 뿐만 아니라, 일반적인 질의 응답을 넘어서 간단한 추론, 연산, 프로그래밍 등의 같은 다양한 작업들을 수행할 수 있기 때문으로 보입니다. 그러나 ChatGPT에도 여전히 극복해야할 다양한 문제점들이 남아있으며, 대표적인 과제 중 하나는 바로 언어 모형의 신뢰성Reliability에 대한 부분입니다. 이 글에서는 언어 모형이 사실에 기반하지 않은 내용을 마치 사실처럼 지어내는 환각 현상Hallucination에 대해 알아보고, 이를 완화하기 위해 어떤 방법들이 연구되어 왔는지 간단하게 리뷰해보도록 하겠습니다. 언어 모형의 환각 현상 Hallucination of Language Models보다 신뢰할만한 답변을 생성하기 위해서, 언어 모형은 1) 사실 정보를 알고 있어야 하며, 2) 이에 기반하여 답변을 생성할 수 있어야 합니다. 여기에서는 Petroni et al. (EMNLP 2019)이 대표적인 언어 모형들 중 하나인 BERT-Large의 답변을 분석한 결과를 통해, 언어 모형이 무엇을 알고 있는지, 잘못된 답변을 한다면 그 이유는 무엇인지에 대해 살펴보도록 하겠습니다. 매우 유명한 언어 모형 중 하나인 BERT는 해당 논문에서 주어진 컨텍스트에 대해 다음과 같은 단어를 예측했습니다. iPod Touch is produced by Apple. London Jazz Festival is located in London. Dani Alves plays with Santos. Carl III used to communicate in German. Ravens can fly. 제시된 예시 중에서 1, 2, 5번에서는 정확한 단어를 3, 4번에서는 잘못된 단어를 생성한 것을 확인할 수 있습니다. 기본적으로, 현재의 기준에서 보면 그리 크지않은 (cf. BERT-Large (345M) vs. PaLM (540B), GPT3 (175B), LLaMa(65B), etc.) 언어 모형임에도 불구하고 별도의 Fine-tuning 과정 없이도 정답을 꽤 잘 맞춘다는 점이 인상 깊습니다. 우리가 조금 더 주목할만한 부분은, 틀린 답변을 하는 경우에도 꽤 그럴듯한 단어를 선택한다는 점입니다. 예를 들어, 세번째 문장에서 Dani Alves는 Barcelona 소속의 축구선수이므로 정답은 Barcelona가 되어야 합니다. BERT가 예측한 Santos는 정답은 아니지만 축구팀 중 하나이므로, 문장 자체만 놓고 본다면 그럴듯한 문장이 되죠. 네번째 문장에서 등장하는 Carl 3세는 19세기 초 스웨덴과 노르웨이를 통치했던 군주로, 독일어가 아닌 스웨덴어를 사용했습니다. 하지만 독일어와 스웨덴어 모두 언어의 일종이므로, 앞선 예시와 마찬가지로 의미 상으로는 문제가 없는 문장이 된다는 점을 살펴볼 수 있습니다. 사실에 근거하지 않은 이러한 답변들이 생성되는 이유는 무엇일까요? Petroni et al.은 다음과 같은 세 가지 항목을 원인으로 제시합니다. 훈련 과정에서 학습한 적 없는 사실 희귀한 사건 언어 모형의 단어 민감성 다음 절에서는 이러한 Hallucination 문제를 해결하기 위해, 즉, 1) 언어 모형에 지식을 통합하고, 2) 언어 모형이 통합된 지식을예측에 적극적으로 활용하도록 하기 위한 다양한 방법들에 대해 살펴보도록 하겠습니다. 언어 모형에 지식을 통합하기 위한 방안 Integration of Knowledge in LM이번 절에서는 연구되었던 방법들을 크게 세 가지 방법론으로 분류하여 살펴보도록 하겠습니다. 각각의 방법론은 1) 사전 학습된 개체 임베딩의 추가, 2) 외부 메모리의 사용, 3) 학습 데이터에 대한 수정입니다. 사전 학습된 개체 임베딩의 추가 Adding Pre-trained Entity Embeddings가장 먼저 살펴볼 모형은 ERNIE (Zhang et al., ACL 2019) 입니다. ERNIE는 기본적으로 지식Knowledge이 여러 개체들Entities 사이의 관계Relation로 표현될 수 있다는 생각에 기반합니다. 예를 들어, ‘Dani Alves’와 ‘소속 팀 관계’에 있는 것은 ‘FC Barcelona’이고, ‘Carl 3세’와 ‘모국어 관계’에 있는 언어는 ‘스웨덴어’라고 할 수 있습니다. 이에 따라, ERNIE는 언어 모형이 언어를 학습함에 있어서 단어에 대한 정보 뿐만 아니라, 단어가 가리키고 있는 개체Entity에 대한 정보를 함께 학습함으로써 지식을 언어 모형에 통합할 수 있다고 주장합니다. 그리고 단어가 전달하는 정보들이 단어 임베딩Word Embedding을 통해 수치 벡터로 표현되는 것처럼, 개체에 대한 정보를 담고 있는 개체 임베딩Entity Embedding을 ERNIE의 학습과정에서 활용하고자 합니다. 단어의 의미를 벡터 공간에 나타내기 위한 많은 노력들(e.g., Word2Vec, BERT)이 있었던 것과 유사하게, 이러한 개체Entity와 관계Relation를 벡터 공간 상의 위치시킴으로써 수치로 표현하기 위한 다양한 방법들이 개발되어 왔습니다. 대표적으로 TransE, Wikipedia2Vec, BLINK와 같은 방법들이 있지만, 이 글에서는 자세히 다루지 않습니다. 다만 ERNIE는 개체 임베딩을 생성하는 방법 자체에 대해서는 Agnostic한 모형이므로, 어떤 개체 임베딩이라도 사용할 수 있다는 특징이 있다는 점에 대해서는 주목할 필요가 있습니다. 그러나 훈련 과정에서 단어 임베딩과 개체 임베딩을 동시에 사용하는 데에는 문제가 하나 있습니다. 바로 두 임베딩이 정의된 벡터 공간이 서로 다르다는 점입니다. 다시 말해, 해당 임베딩들은 일반적으로 서로 독립적으로 학습된 벡터들이므로, 특정 벡터가 각 공간에서 지니는 의미가 전혀 다를 수 있다는 점입니다. 예를 들어, 단어 임베딩은 768 차원에서 정의되고, 개체 임베딩은 128 차원에서 정의되는 것처럼 두 임베딩이 정의되는 차원 자체가 아예 다를 수도 있습니다. 따라서, 각각의 임베딩을 두 임베딩이 함께 사용될 수 있는 새로운 벡터 공간으로 맵핑시켜주는 과정이 필요한데, 해당 논문에서는 이러한 과정을 Fusion이라고 부르고 있습니다. 이러한 Fusion은 각각의 임베딩에 Fusion Matrix를 곱해줌으로써 이루어지며, 해당 Matrix 또한 학습을 통해 최적화되는 파라미터입니다. 마지막으로, ERNIE에서는 언어 모형이 추가된 개체 임베딩을 훈련 과정에서 보다 적극적으로 활용하도록 하기 위해, 각각의 Input token이 어떤 개체를 가리키는지 예측하는 Task를 훈련 과정에 추가하였습니다. denoising Entity Autoencoding (dEA)라고 불리는 해당 Task는 Vincent et al. (2008)에 의해 처음 제안되었으며, 기존의 BERT의 Loss에 $\\mathcal{L}_{dEA}$가 추가된 ERNIE의 최종 Loss는 다음과 같은 형태가 됩니다. $$ \\mathcal{L}_{ERNIE} = \\mathcal{L}_{MLM} + \\mathcal{L}_{NSP} + \\mathcal{L}_{dEA}$$ 외부 메모리의 사용 Using an External Memory다음으로 살펴볼 방법은 External Memory를 활용하는 방법입니다. Khandelwal et al. (2020)이 제안한 KNN-LM은 언어 모형의 단어 민감성에 주목합니다. 오답을 생성했던 언어 모형은, 질문에 사용된 단어를 약간만 다르게 변경해줘도 정답을 맞추는 경우가 많습니다. 예를 들어, 언어 모형이 의미론적으로 유사한 세 문장에 대해 다음과 같은 결과를 생성하는 경우에 대해 생각해보겠습니다. Carl III used to communicate in German. Carl III used to speak in Swedish. Carl III’s mother tongue was Swedish. 첫 번째 문장에서는 언어 모형이 정답을 맞추지 못했지만, 두번째와 세번째 문장에서는 올바른 단어를 예측한 것을 확인할 수 있습니다. 그렇다면 다음 토큰을 예측할 때에, 주어진 문장과 의미론적으로 유사한 다른 문장들의 예측 결과를 활용하면 예측의 안정성을 향상시킬 수 있지 않을까요? 원 논문의 다이어그램을 보면서 조금 더 자세히 살펴보도록 하겠습니다. 상단의 다이어그램에서 우리는 ‘Shakespeare’s play ____’라는 문장에서 빈칸에 해당하는 단어를 예측하고자 합니다. 해당 문장만 가지고 예측을 수행할 경우 빈칸에 해당할 확률이 가장 높은 단어는 0.2의 확률을 지니는 Hamlet 이라는 것을 알 수 있습니다. 반면, 앞서 언급했던 것처럼 KNN-LM은 해당 문장과 의미론적으로 유사한 문장들의 예측 결과를 추가적으로 활용하고자 했습니다. 기본적으로 KNN-LM은 KNN을 활용하여 얻은 예측 결과와 LM을 통해 얻은 예측 결과를 통합하는 형태로 구성되어 있는데, 이러한 답변 생성은 다음과 같은 절차를 통해 이루어집니다. 우선, KNN-LM은 예측하고자 하는 문장과 유사한 k개의 문장들을 사전에 구축된 Datastore에서 검색합니다. 문장들 간의 유사도는 각 문장의 Vector representation에 다양한 Distance/Similarity Measure를 적용하여 계산할 수 있습니다. 상단의 예시에서는 ‘Shakespear authored ____’, ‘Shakespeare wrote ____’, ‘The Bard’s famous play ____’의 세 문장이 각각 Distance 3, 4, 5를 지니는 유사 문장으로 선별된 것을 확인할 수 있습니다. 다음으로, KNN-LM은 유사한 문장들 다음에 왔던 단어들에 대해 살펴보고, 이를 종합하여 KNN에 의한 토큰 선택 확률을 계산합니다. Hamlet이 가장 높은 확률로 예측되었던 LM에 의해 예측과는 다르게, KNN을 통한 예측에서는 Macbeth가 0.8의 높은 확률을 지니는 다음 단어로 예측된 것을 확인해 볼 수 있습니다. 마지막으로, KNN-LM의 최종 예측은 Macbeth, Hamlet이 각각 0.8, 0.2의 확률을 지녔던 KNN 파트의 예측과 0.1, 0.2의 확률을 보였던 LM 파트의 예측 결과를 Interpolation 하는 형태로 수행됩니다. 주어진 다이어그램에서는 Macbeth가 0.7, Hamlet이 0.2의 최종 확률을 지님으로써, Hamlet이 선정되었던 기존의 LM의 결과와는 다르게 Macbeth가 선택되는 결과를 보여주고 있습니다. 학습 데이터에 대한 수정 Modifying the Training DataXiong et al. (ICLR 2020)은 언어 모형이 올바른 지식과 잘못된 지식을 명시적으로 훈련받은 적이 없다는 사실에 주목합니다. 예를 들어, 언어 모형의 훈련 데이터에 ‘Dani Alves has played in FC Barcelona.’라는 문장이 포함되어 있었다고 생각해보겠습니다. 물론 언어 모형은 해당 문장을 통해 Barcelona라는 단어가 선택될 확률이 높아지도록 학습을 진행하지만, Santos라는 단어로 예측을 하면 안된다는 점을 명시적으로 학습하지는 않습니다. 언어 모형에게 의도적으로 ‘오답’을 보여주고, 해당 오답을 선택하지 않도록 명시적으로 학습을 진행하면 모형이 오답률을 낮출 수 있지 않을까요? 이러한 가정에 따라, 해당 논문에서는 학습 데이터셋 자체를 의도적으로 오염corrupted시키고, 언어 모형이 학습을 진행할 때 해당 단어가 잘못된 정보로 오염된 단어인지 아닌지 예측하는 Task를 추가한 모형인 WKLMWeakly Supervised Knowledge-Pretrained Language Model을 제안합니다. WKLM에서 추가된 이진 분류 Task에 대한 성능은 Entity Replacement Loss $\\mathcal{L}_{ER}$에 따라 평가되며, 이에 따라 WKLM의 최종 Loss는 다음과 같은 형태가 됩니다. $$ \\mathcal{L}_{WKLM} = \\mathcal{L}_{MLM} + \\mathcal{L}_{ER} $$ 마치며 Outro이번 글에서는 언어 모형의 Hallucination 현상을 완화하기 위한 다양한 방법들에 대해 살펴보았습니다. 언어 모형의 신뢰성 향상은 현재도 활발히 연구되고 있는 분야입니다. 기회가 된다면 다음에는 OpenAI의 RETRO (Borgeaud et al, 2021) 등 상대적으로 최근에 제안된 방법론들에 대해 살펴보도록 하겠습니다.","link":"/Knowledge%20Integration%20in%20Language%20Model/"},{"title":"Learning Frameworks in NLP (1&#x2F;2)","text":"1. Intro최근에 KoLIMA라는 사이드 프로젝트를 하나 시작했습니다. 2023년 5월에 Meta AI에서 발표한 LIMA: Less Is More for Alignment라는 모형에서 사용한 방법론이 한국어 언어 모형에도 적용 가능한지 확인해보고자 하는 목적에서 진행하는 프로젝트입니다. 1.1 LIMA and KoLIMA우선 LIMA는, 소규모의 엄선된 instruction dataset 만으로도 pre-training objective와 final objective 사이의 mismatch를 충분히 align 할 수 있다는 내용을 다루고 있습니다. 우리가 언어 모형을 연구하고 개발하는 이유는 무엇일까요? 다르게 표현하자면, 사람들이 언어 모형에게 기대하는 바는 무엇일까요? 아마도 우리는 언어 모형이 우리의 요청에 따라 질문에 올바르게 답변하고, 번역이나 요약 등의 요청도 적절히 수행하기를 바랄 것입니다. 즉, 우리가 언어 모형을 학습시키는 최종 목적은 ‘언어 모형Language Model이 입력값Input으로 주어지는 사람의 지시Instruction을 따르게 하는 것’이라고 할 수 있습니다. 한편, 우리가 이미 살펴보았듯이 언어 모형의 사전 학습Pre-training 단계에서의 학습 목표는 ‘Masked/Causal Language Modelling의 Loss의 최소화’입니다. 따라서, ‘우리의 최종 목표와 언어 모형의 훈련 단계에서의 학습 목표 사이에는 Mismatch(혹은 Gap)가 존재하는데, 이러한 상이한 학습 목표를 어떻게 Align 할 수 있을까?’라는 자연스러운 질문은 최근 NLP 분야에서 많은 관심을 받아온 연구 주제입니다. 예를 들자면, ChatGPT의 전신이자 Sibling 모형인 InstructGPT도 해당 문제를 다루고 있고, LLaMA 모형에 (Self-instruct를 활용하여 생성한 Instruction dataset을 통해) Instruction-tuning을 적용한 Stanford Alpaca도 동일한 주제에 관심을 집중하고 있습니다. 많은 분들이 아시다시피, ChatGPT 훈련 과정의 핵심은 RLHF를 통해 Human Feedback을 학습 과정에 반영하는 것이지만, 해당 프로세스는 꽤 높은 비용을 필요로 합니다. 물론 OpenAI도 RLHF를 최대한 자동화된 프로세스로 만들기 위해서 노력하긴 했습니다. Human Feedback을 예측하는 모형을 별도로 훈련시켜서 활용하는 구조로 구성했기 때문에, 일정 시점 이후부터는 사람이 언어 모형의 아웃풋에 대한 Feedback을 직접 Annotation 할 필요성이 줄어들긴 합니다. 그럼에도 불구하고 RLHF는 전체적으로 고비용의 프로세스일 수 밖에 없을 뿐더러, OpenAI가 학습에 사용한 데이터셋을 공개하지 않았기 때문에 데이터와 모형의 재활용을 통한 비용 절감 또한 기대하기 어려운 상황이죠. 개인적으로 생각하기에, LIMA는 ‘그게… 그렇게까지 할 일인가?’라는 질문에서부터 출발했던 것 같습니다. (Xie et al., 2021)과 (Ahuja et al., 2023)을 비롯한 여러 논문들의 실험 결과가 암시하듯이, In-context learning ability를 비롯한 거대 언어 모형의 대부분의 지식은 이미 사전 학습 단계에서 학습된 것이며, instruction-tuning은 단지 사람과 언어 모형이 상호 작용하는 방식을 학습하는 간단한 과정일 가능성이 높습니다. 따라서 ‘RLHF처럼 복잡한 과정 없이도, 1k 규모의 엄선된 instruction dataset만으로도 만족할만한 성능의 alignment를 수행할 수 있다’라는 내용이 LIMA의 저자들이 주장하고자 했던 핵심적인 내용입니다. 앞서 언급했듯이, KoLIMA는 LIMA의 저자들이 주장하는 내용이 한국어 언어 모형에서도 동일하게 적용되는지 확인해보기 위한 프로젝트입니다. 조금 더 구체적으로는, 아래 두 가지의 항목에 대해 살펴보는 것을 목적으로 합니다. 영어와 한국어의 언어적 특성에 따른 instruction-tuning의 효과 차이 영어로 작성된 instruction dataset을 기계 번역을 통해 한국어로 변환하는 과정에서 필연적으로 발생하는 noise(번역 오류 등)에 대한 강건성 이를 위해, 우선 LIMA dataset을 DeepL API를 활용하여 한국어로 번역한 KoLIMA dataset을 생성하고, 사전 학습된 한국어 언어 모형인 Polyglot-ko을 백본 모형으로 instruction-tuning을 적용하여 모형의 성능을 평가해보고자 합니다. 현재 데이터셋의 번역은 완료된 상태이고, Polyglot-ko 1.3B, 3.8B 모형의 학습에 이어서 5.8B, 12.B 모형의 학습을 진행 중에 있습니다. 1.2 This Article Covers:이번 글에서는 지금까지 언급했던 내용들인 pre-training, fine-tuning, instruction-tuning, transfer learning 등의 개념을 이해하기 위한 배경 지식에 대해 다룹니다. 이전 글들에서 다루었던 CNNs, RNNs, Transformers 등의 개념이 LM을 구성하는 아키텍쳐 측면에서의 구성 요소였다면, 이번 글에서는 LM을 ‘어떻게 학습시킬 것인가’에 대한 내용을 다룹니다. 다시 말해, 최근 몇 년간의 NLP 분야에서의 연구 트랜드를 Learning Framework의 관점에서 정리해보도록 하겠습니다. 또한, 이 글은 Lena Voita와 Sebastian Ruder의 블로그를 참고하고, 많은 영감을 받았음을 밝힙니다. 2. Learning Frameworks in NLP2.1 Supervised Learning with Task-specific Models머신러닝을 공부하기 시작하면 가장 먼저 배우게 되는 개념들 중 하나가 바로 지도 학습과 비지도 학습에 대한 내용입니다. 지도 학습은 레이블이 붙어있는 학습 데이터셋을 활용하여 모형이 특정 Task를 수행하는 방법을 가르치는 훈련 방법이죠. 그리고 얼마 전까지만해도 대부분의 NLP Task들은 해당 Task를 수행하기 위한 구체적인 모형이 있고, 해당 모형에 지도 학습 적용하여 훈련시키는 방법을 사용했었습니다. 예를 들어, 텍스트 요약, 주제 분류, 번역과 같은 3개의 구체적인 task가 있다고 할 때, 각각의 task를 위한 세 개의 모형이 별도로 존재하는 형태였죠. 그림으로 표현해보자면 다음과 같습니다. 2.2 Pre-trained Models as Auxiliary Model2.2.1 Pre-trained Model for General Knowledge그런데, 텍스트 요약과, 주제 분류, 번역이라는 세 가지 Task를 잘 하기 위해서 필요한 공통적인 지식이 있을 수 있지 않을까요? 예를 들어, 한국어를 할 줄 모르는 사람에게 한글로된 뉴스 기사를 요약하고, 주제를 분류하고, 영어로 번역하는 것을 가르치는 것보다, 한국어를 이미 알고 있는 사람에게 동일한 내용을 가르치는 것이 훨씬 더 효율적일 것입니다. 이렇게 특정 Task Specific Task에 관계없이 일반적으로 활용될 수 있는 언어에 대한 지식을 일반 지식General Knowledge이라고 하고, 여기에는 개별 단어의 의미, 품사, 문장 내에서의 역할, 문법 구조 등이 포함됩니다. 이러한 일반 지식은 레이블이 필요하지 않은 비지도 학습 기법을 통해서도 학습시킬 수 있다는 사실이 선행 연구들을 통해 알려져있고, 특정 Task를 위한 레이블링이 되어 있지 않은 대량의 텍스트 데이터를 통해 이러한 일반 지식을 학습시킨 모형을 우리는 사전 학습 모형Pre-trained Model이라고 부릅니다. 사전 학습 모형의 일반 지식이 각각의 Task를 위한 모형Task-specific Models에서 활용되는 구조를 그림으로 나타내면 다음과 같습니다. 그렇다면, 이러한 일반 지식을 어떤 형태로 개별 모형들에게 전달할 수 있을까요? 가장 일반적인 방법은 단어 임베딩Word Embeddings에 이러한 정보들을 담아 개별 모형의 입력값으로 활용하는 것입니다. 혹은 지난 글에서 살펴봤던 것처럼 개체 임베딩Entity Embeddings을 활용할 수도 있습니다. 이러한 임베딩을 생성하기 위한 다양한 방법들이 연구되어 왔으며, 대표적으로는 word2vec, ELMo, BERT 등이 있습니다. 각각의 방법론이 어떤 특징을 지니고 있는지 간략히 살펴보고, Learning Framework의 관점에서 어떤 방식으로 활용되는지 다시 한 번 정리해보도록 하겠습니다. 2.2.2 Era of Kings and Queens: Static Embeddings이번 섹션의 소제목인 ‘왕과 여왕들의 시대 Era of Kings and Queens‘는 많은 분들께 익숙할 vector(King) - vector(Man) + vector(Woman) = vector(queen)의 예시로부터 가져왔습니다. 해당 예시는 word2vec을 제안했던 Tomas Mikolov의 논문에서 처음 등장했으며, word2vec은 ‘함께 등장하는 단어 집합이 유사한 단어들은 서로 비슷한 의미를 지닌다’라는 간단한 가정을 기반으로 단어를 수치 벡터로 표현하는 방법입니다. (Mikolov et al., 2013)은 CBOW 혹은 Skip-gram이라는 방법을 통해 동시 발생 빈도를 기반으로 각 단어의 의미를 연속 공간Continuous Space에 표현하고, 이러한 단어 벡터(임베딩)가 단어들 간의 관계를 일정 수준에서 포착Capture할 수 있다는 사실을 보였습니다. 그 이후로도 단어의 의미를 연속 공간에서 표현하기 위한 많은 연구가 이루어졌으며, Winodw size로 인한 한계를 보완하기 위한 방법인 Glove (Pennington et al., 2014)와 Subword의 개념을 활용한 FastText (Bojanowski et al., 2016) 등이 추가로 제안되었습니다. 다만 word2vec을 비롯한 이러한 방법론들은 주변에 함께 등장한 단어를 고려하지 못하고, 하나의 단어에 문맥에 관계없이 고정된 하나의 임베딩만을 할당한다는 단점이 있습니다. 다시 말해, word2vec을 통해 생성한 임베딩을 통해서는 이 글에서 다루었던 여러 의미를 지닐 수 있는 ‘배’를 각각 다르게 표현할 수 없다는 한계를 지닙니다. 우리는 이러한 특징을 지니는 임베딩을 다음 장에서 살펴볼 동적 임베딩Contextualised/Dynamic Embeddings과 구분하는 의미에서 정적 임베딩Static Embeddings이라고 부릅니다. 2.2.3 ELMo: Contextualised Word EmbeddingsELMo는 이러한 \b정적 임베딩의 단점을 개선한 동적 임베딩Dynamic Embeddings을 생성하는 방법론입니다. 주변 문맥을 반영한 임베딩이라는 의미에서 문맥 임베딩Contextualised Embeddings라고 표현하기도 합니다. 여기서 ‘동적’이라는 표현은 주변에 등장한 단어(문맥Context)에 따라서 임베딩이 표현하는 단어의 의미가 변화한다는 특징을 나타내기 위한 표현입니다. 아래의 Lena Voita의 Figure를 통해, 동적 임베딩이 정적 임베딩으로부터 어떤 점이 다른지 살펴보도록 하겠습니다. Figure의 좌측은 정적 임베딩을 나타내고 있습니다. 정적 임베딩에서 ‘cat’이라는 단어는 주변에 함께 등장한 단어들과 관계없이 항상 동일한 수치 표현(벡터)를 갖습니다. flying cat의 cat도, the cat on the mat의 cat도, the cat by the door의 cat도 모두 동일한 수치 벡터로 표현됩니다. 한편, ELMo (Peters et al., 2018)를 비롯한 동적 임베딩 기법에서는 각각의 cat을 주변 맥락Context을 고려한 임베딩으로 표현할 수 있습니다. 다시 말해, 앞선 예시의 서로 다른 세 구문에서의 cat은 모두 다른 수치 벡터로 표현되며, 이러한 특징은 상단 그림의 우측에서 확인할 수 있습니다. 이러한 동적 임베딩은 주변 맥락에 따라 달라지는 의미를 표현할 수 있을 뿐만 아니라, 동음이의어와 같이 하나의 단어가 여러 의미를 지니는 현상을 다루기에도 적합한 방법론이라고 할 수 있습니다. 2.2.4 Pre-trained Embeddings as an Input for Task-specific Models지금까지 살펴본 것처럼, 정적 임베딩을 생성하는 word2vec, GloVe, FastText 등의 방법론과 동적 임베딩을 생성하는 ELMo는 단어를 표현할 때 주변 맥락을 고려할 수 있는지 없는지에 따른 차이가 있긴 하지만, 생성된 임베딩이 최종 Task를 위해 활용되는 방법에 있어서는 차이가 없습니다. 다시 말해, 아래 그림에서 살펴볼 수 있는 것처럼 사전 학습 모형은 단어 집합을 입력으로 받아 각각의 단어에 대한 임베딩을 생성하고, 이는 다시 개별 Task를 위한 모형Task-specific Model의 입력값으로 활용됩니다. 이러한 관점에서, 우리가 최종적으로 수행하고자 하는 작업을 위한 개별 모형을 메인 모형Main Model, 메인 모형에서 활용하기 위한 일반 지식을 전달하기 위한 사전 학습 모형을 보조 모형Auxiliary Model이라고 부르기도 합니다. 그리고 각각의 작업을 위한 개별 모형을 메인 모형으로 보는 이러한 관점은 GPT-2와 BERT를 기점으로 크게 변화하게 되는데, 다음 절에서는 이러한 관점 변화에 대해서 살펴보도록 하겠습니다. 2.3 Pre-trained Models as Universal Model2.3.1 Universal Models with Contextualised EmbeddingsBERT (Devlin et al., 2019)는 이전 절에서 살펴본 ELMo와 마찬가지로 각 단어에 대한 동적 임베딩을 생성하는 모형입니다. 그러나 이번 절에서 우리가 주목할 부분은 BERT가 Task를 수행하기 위해 활용되는 방법입니다. 결론부터 이야기하자면, BERT는 단일 모형으로 여러 Task를 수행할 수 있는 범용 모형Universal/Foundational Model입니다. 각각의 Task를 위한 task-specific model이 별도로 존재하고 사전 학습 모형으로부터 생성된 임베딩은 해당 모형의 입력으로 활용되던 기존의 프레임워크와 달리, BERT를 제안한 Devlin et al.은 질의응답Question Answering, 시퀀스 태깅Sequential Tagging, 문장 혹은 문장쌍 분류Sentence (Pair) Classification 등의 다양한 task를 수행하기 위해 별도의 모형을 사용하지 않았습니다. 대신, BERT로부터 생성된 임베딩 위에 존재하는 최상단 레이어만 교체해가면서 여러 Task를 수행하는 것만으로도, 기존의 각각의 모형들보다 개선된 성능을 확보할 수 있다는 사실을 보였죠. 이러한 구조를 그림으로 표현하자면 다음과 같습니다. 더 이상 각각의 Task를 위한 개별 모형이 존재하지 않고, 하나의 사전 학습 모형을 통해 요약, 분류, 번역 세 가지 작업 모두를 수행한다는 점에 주목할 필요가 있습니다. 이는 모형의 학습 프레임워크 관점에서 혁신이라고 불릴만한 발견이었습니다. 이러한 새로운 프레임워크에서 사전 학습 모형은 더 이상 메인 모형에서 활용되기 위한 보조 모형Auxilary Model이 아니라, 하나의 모형만으로도 여러 Task를 수행할 수 있는 Multi-task 모형이자 메인 모형으로 기능합니다. 이러한 연구 트랜드의 변화는 GPT-2 (Radford et al, 2018)의 논문 제목에서도 살펴볼 수 있는데요, 논문 제목인 ‘Language Models are Unsupervised Multitask Learners‘에서는 GPT-2의 아키텍쳐 변화도, 모형의 크기 증가도 아닌 Multi-task를 수행할 수 있는 능력을 강조하고 있습니다. 2.3.2 Welcome to Sesame Street!그리고 GPT-2와 BERT를 필두로 NLP 연구 분야는 Sesame Street 친구들이 점령합니다. Grover (Zellers et al., 2019), ERNIE (Zhang et al., 2019), BART (Lewis et al., 2019), Kermit (Zanzotto et al., 2019), Big Bird (Zaheer et al., 2020) 등 다양한 친구들이 등장했죠. 국내에서는 카카오 엔터프라이즈가 Little Bird (Lee et al., 2022)라는 모형을 발표하여 KorQuAD 2.0 챌린지에서 1위를 달성하기도 했습니다. 한편으로는 BERT를 개선한 많은 모형들이 연구되기도 했습니다. 이러한 모형들의 예시로는, Dynamic Masking의 적용과 NSP Loss 제거 등을 통해 조금 더 Optimimal한 성능을 확보하고자 했던 RoBERTa (Liu et al., 2019), 상대적으로 가벼운 아키텍쳐를 갖고 있는 ALBERT (Lan et al, 2019), 언어 구조를 동시에 학습시킨 StructBERT (Wang et al., 2019) 등이 있습니다. 마지막으로, 범용 모형Universal/Foundational Model에 대해 조금 더 자세히 알아보고 싶은 분들께 ‘On the Opportunities and Risks of Foundation Models (Bommasani et al., 2021)’이라는 논문을 권해드리며 이번 \b절을 마무리하고자 합니다. 3. Outro이번 글에서는 Sequential Transfer Learning의 기본적인 개념인 사전 학습Pre-training과 미세 조정Fine-tuning, 그리고 사전 학습 모형의 역할이 어떻게 변해왔는지에 대해 살펴보았습니다. 다음 글에서는 Few/Zero-shot Learning, Prompting, In-context Learning, Instruction-tuning 등의 개념에 대해 살펴보도록 하겠습니다.","link":"/LF-NLP-1/"},{"title":"Language Models","text":"결론부터 이야기하자면, 언어 모델language model은 각 문자열에 확률을 부여한 확률 모형probabilistic model of strings이고, 언어 모델링language modelling은 이러한 확률을 부여하기 위한 과정을 의미합니다. 이 글에서는 어떠한 맥락에서 이러한 정의가 성립하는지 살펴보도록 하겠습니다. 우리가 한국어를 할 줄 안다는 것은 무엇을 의미할까요? 친구가 우리에게 ‘오늘 뭐 해?’라고 물어본 상황을 한 번 가정해보겠습니다. 우리가 질문을 정확히 이해했다면 아마 다음의 (1), (2)번과 같은 대답들을 떠올릴 수 있을 것입니다. 한편, 동일한 질문에 대해서 (3)번이나 (4)번처럼 대답할 일은 별로 없겠죠. 오늘은 집에서 좀 쉬려고! 응, 너랑은 안놀아. 어제 피자 먹었어. 나는 요즘 박보영이 좋더라. 이를 확률의 관점에서 이야기 해보자면, 우리가 ‘오늘 뭐 해?’라는 질문을 받았을 때 ’오늘은 집에서 좀 쉬려고!’ 혹은 ‘응, 너랑은 안놀아.’라는 문장을 선택할 확률은 ‘어제 피자 먹었어.’나 ‘나는 요즘 박보영이 좋더라.’라는 문장을 선택할 확률보다 훨씬 더 높아야 한다는 의미입니다. 즉, ‘언어를 이해하고 사용한다’라는 개념을 매우 거칠게 정의하자면, 해당 언어에서 각 문자열들의 발생 확률을 추측하는 과정이라고 볼 수 있습니다. 이러한 맥락에서, 우리는 모든 후보 문자열에 대한 발생 확률을 추정하는 과정을 언어 모델링language modelling, 그러한 과정을 통해 생성된 문자열들의 확률 분포를 언어 모형language model이라고 부릅니다. 확률의 추정 — MLE앞서 우리는 언어 모델링을 각 문자열에 대한 발생 확률을 추정하는 과정으로 정의했습니다. 그러면 이러한 확률들은 어떻게 추정할 수 있을까요? 가장 쉽게 떠올릴 수 있는 방법은 각 문장들의 발생 빈도를 기반으로 하는 것입니다. 예를 들어, 우리에게 10000 문장의 대화 데이터가 주어졌는데, 그 중에서 200 문장이 ‘안녕’이라는 문장이라면 해당 문자열string의 발생 확률은 200/10000 = 2% 정도 될 것이라고 추정해 볼 수 있는 것이죠. 주어진 데이터에 기반하여 가능도likelihood가 최대가 되도록 확률을 추정하는 이러한 방법을 최대 가능도 추정법MLE, Maximum Likelihood Estimation이라고 부릅니다. 확률론에 기반하고 있는 추정 방법이지만 이 글에서는 깊게 다루지 않습니다. 단순히 ‘문자의 발생빈도/전체빈도’를 기준으로 각 문자열의 발생 확률을 추정하는 방법이라고만 이해하고 넘어가도록 하겠습니다. 빈도를 기반으로 확률을 추정할 수 있게 된 것은 좋은데, 문장 레벨에서 발생 빈도를 카운트 하다보면 발생하는 문제가 있습니다. 우선, 우리가 방금 살펴본 MLE는 각 문장의 발생 확률을 ‘정확한 법칙에 따라 계산’하는 것이 아니라 ‘데이터에 기반하여 추정’하는 방법이라는 사실을 기억할 필요가 있습니다. 또한 MLE에서는 기본적으로 주어진 데이터(샘플)가 전체 언어(모집단)을 대표하기에 충분하다는 것을 가정합니다. 따라서 전체 언어를 대표할 수 있을만한 충분한 데이터를 수집하는 것은 무엇보다 중요한 일이 되는데, ‘언어의 변동성’은 이러한 데이터를 수집하는 것을 매우 어렵게 만듭니다. 다음 단락에서는 ‘언어의 변동성’이 무엇인지 살펴보도록 하겠습니다. 언어의 변동성 A Possible Variability of Languages언어가 가지는 기본적인 성질들 중 하나인 ‘변동성’은, 언어의 변화 가능성이 무한하다는 것을 의미합니다. 즉, 동일한 의미를 지니는 하나의 문자열은 여러 형태를 지닐 수 있다는 의미입니다. 예를 들어, 앞서 살펴보았던 ‘오늘은 좀 쉬려고!’라는 문장은, 아래와 같이 동일한 의미를 가지는 수없이 많은 문장들로 고쳐쓸 수 있습니다. 1-1) 집에서 오늘은 좀 쉬려고!1-2) 오늘은 집에서 조금 쉬려고!1-3) 오늘 그냥 집에서 좀 쉬려고!1-4) 오늘은 좀 쉴래! 문제는, 이러한 형태 변화가 무한히 가능하기 때문에, 우리가 언어 모델링을 위해 아무리 많은 데이터를 수집한다고 하더라도 다양한 형태들 중 일부만을 포함할 수 밖에 없다는 데에 있습니다. 다시 말해, 언어의 변화 가능성은 무한한 데에 반해 우리의 데이터는 한정적일 수 밖에 없으므로, 언어 모델링에 사용되는 데이터는 전체 언어를 충분히 대표하지 못하게 됩니다. 따라서, 우리가 수집한 데이터에는 나타나지 않은 문장들의 실제 발생 확률은 0이 아님에도 불구하고, MLE 기반의 확률 추정에서는 해당 문장들의 발생 확률을 모두 0으로 추정하는 문제가 발생합니다. 예를 들어, 우리가 수집한 데이터에 (1) 오늘은 좀 쉬려고!라는 문장이 있었더라도, 문장 레벨에서의 MLE에서는 동일한 의미를 지니고 있는 1-1, 1-2, 1-3, 1-4 문장의 발생 확률을 0으로 추정할 수도 있습니다. 그리고 이러한 문제는 문장 레벨에서 발생 빈도를 카운트할 때 더욱 두드러지게 되는데, 정확하게 동일한 문장이 관측될 확률은 동일한 단어가 관측될 확률에 비해 상대적으로 작을 수 밖에 없기 때문입니다. 또 다른 예시로 ‘사랑하다’라는 의미의 ‘love’라는 영어 단어를 한번 살펴볼까요? 해당 단어는 주어의 단/복수 여부에 따라, 시제에 따라, 품사 변화에 따라 loves, loved, loving 등 매우 다양한 형태의 형태로 변화할 수 있습니다. 이러한 단어의 형태 변화를 굴절/파생 형변환inflectional/derivational morphology이라고 부르며, 언어의 무한한 변동성의 하나의 원인이 됩니다. 단어 레벨에서의 변동성을 완화하기 위한 방법으로는 어간 추출stemming과 표제어 추출lemmatization과 같은 방법들을 사용할 수 있습니다. 정리해보겠습니다. 언어는 단어 레벨에서의 형 변환과 문장 레벨에서의 단어 조합으로 인해 무수히 많은 변화 가능성을 지니고 있습니다. 이러한 언어의 변동성은 우리가 수집한 데이터가 전체 언어를 정확하게 대표하는 것을 불가능하게 만들고, 이는 MLE를 통해 정확한 확률을 추정하는 것을 어렵게 만듭니다. 단어 레벨에서의 형 변환은 어간 추출이나 표제어 추출과 같은 방법을 통해 완화될 수 있으나, 이 글에서는 자세히 다루지 않습니다. 문장 레벨에서의 단어 조합으로 인한 변동성은 문장을 단어들의 조합으로 다룸으로써 완화될 수 있으며, 다음 단락에서 자세히 살펴보도록 하겠습니다. 문장 = 단어들의 조합제한된 데이터로 문장의 발생 확률을 예측하려고 하다보니, 우리가 수집한 데이터에 있는 문장들과 조금이라도 다른 문장들의 발생 확률들은 모두 0으로 추정하게 되었습니다. 우리는 ‘문장sentence’을 조금 작은 단위인 ‘단어들의 조합combination of words’라는 관점에서 바라봄으로써 이러한 문제를 다소 완화할 수 있습니다. 즉, n개의 단어로 구성된 문장 $Sentence_n$은 각 단어 $word_n$을 통해 다음과 같이 표현될 수 있습니다. 아래에서 특수문자 ‘;’는 이어붙임concatenation을 나타냅니다. $$ sentence_n = word_1; word_2; word_3; \\ldots; word_n $$ 이러한 관점 전환을 통해, 우리는 이제 문장의 발생 확률을 각 등장 단어들의 발생 확률의 곱셈을 통해 추정할 수 있게 되었습니다. 따라서, 이제는 우리가 확률을 추정하고자 하는 문자열과 정확하게 동일한 문자열이 수집한 데이터에 포함되어 있지 않더라도 0이 아닌 확률을 추정할 수 있습니다. $ P(오늘은\\ 집에서\\ 좀\\ 쉬려고 !) = P(오늘은) \\times P(집에서) \\times P(좀) \\times P(쉬려고) \\times P(!) $ $ P(집에서\\ 오늘은\\ 좀\\ 쉬려고 !) = P(집에서) \\times P(오늘은) \\times P(좀) \\times P(쉬려고) \\times P(!) $ 그러나 이러한 접근법에 장점만 있는 것은 아닙니다. 문장을 단어들의 조합으로 분할하는 과정에서, 단어가 등장하는 순서를 고려할 수 없게 되기 때문입니다. 다음의 두 문장의 예시를 통해 살펴보도록 하겠습니다. 5-1) I am traveling from Seoul to Edinburgh.5-2) I am traveling from Edinburgh to Seoul. 각 문장에서 사용되고 있는 단어는 정확하게 동일하기 때문에, 단어 레벨에서의 MLE를 이용하면 두 문장의 발생 확률은 동일하게 추정됩니다. 문제는, From Seoul과 from Edinburgh, 혹은 to Edinburgh와 to Seoul은 다른 의미를 가지고 있고 발생 확률도 다를텐데, 현재 우리의 언어 모형은 이러한 단어 순서에 따른 차이를 전혀 고려하지 못한다는 점입니다. 지금까지의 내용을 정리하자면, 우리는 MLE를 통해 문자열의 확률을 추정하고 있습니다. 그리고 데이터에서 관측되지 않은 문자열에 대한 발생 확률을 모두 0으로 추정하는 것은 MLE의 한계라고 볼 수 있습니다. 우리는 문장 레벨이 아닌 단어 레벨에서 빈도를 카운트 함으로써 이를 일정 수준 완화할 수 있으나, 더 이상 단어의 발생 순서를 고려할 수 없게된다는 새로운 문제와 마주하게 됩니다. 다음 글에서는 이러한 문제를 완화하기 위한 방법인 n-grams에 대해 살펴보도록 하겠습니다. 요약 언어 모델링은 각 후보 문자열에 확률을 할당하는 과정이며, 언어 모델은 각 문자열의 확률 분포를 나타낸다. 문자열의 발생 확률은 MLE를 통해 추정할 수 있다. 문장 레벨에서의 문자열은 발생 빈도가 너무 낮기 때문에, 단어 레벨로 분할하여 확률을 추정할 수 있다. 문장을 단어 레벨로 분할할 경우 단어의 순서를 고려할 수 없다.","link":"/Language%20Models/"},{"title":"[Taes&#39; R #1] R에서의 분석 프로세스와 일관된 인터페이스","text":"0. 시작하기에 앞서안녕하세요, 지금까지 만든 블로그 수가 작성한 글의 수보다 많은 Taes 입니다. 블로그 운영을 매년 새해 목표로만 간직해오다가, 지금은 소소하게 ‘R과 관련된 포스트 3개 작성해보기’를 목표로 글을 작성하고 있습니다. 제가 학습한 내용들을 정리하는 차원에서 작성하는 글들이므로, 잘못 이해한 부분들이 있을 수 있습니다. 그러한 부분들은 지적해주시면 더욱 성장하는 기회로 삼고 수정할 수 있도록 하겠습니다 :) 첫 번째 글의 주제는 ‘R에서의 분석 프로세스와 일관된 인터페이스’입니다. 1. 공포스러운 사실 ㅡ 통일되지 않은 인터페이스, 그 난잡함에 대하여R은 분명 많은 장점을 가진 언어다. 그 중 하나는 오픈 소스라는 특성 덕분에 수많은 Contributor들에 의해 다양한 최신(cutting edge) 알고리즘들이 빠르게 구현되고, 배포되며, 디버깅되는 생태계가 구성되어 있다는 점이다. 다만 동일한 사유로 인해 파생되는 단점도 있는데, 많은 알고리즘이 서로 다른 라이브러리에서 구현되고, 각각의 라이브러리의 Author가 다르며, 이 때문에 알고리즘을 이용하기 위한 인터페이스도 모두 다르게 개발되고 있다는 점이다. 이야기가 잘 와닿지 않을 수도 있으니, 간단한 예시로 Decision Tree를 구현한 패키지들을 한 번 살펴보자. 간단한 검색으로도 tree, rpart, party 등 여러 종류의 패키지가 존재하는 것을 확인할 수 있다. NOTE: 물론 세부적으로 이용되는 알고리즘에는 다음과 같이 약간의 차이가 있다. tree: Binary recursive partitioning rpart: Classification and regression trees(CART) party: Unbiased recursive partitioning based on permutation tests 앞의 두 개 패키지에서 모형을 fitting하는 함수는 각각 tree()와 rpart()이다. 함수명이 다른 것은 물론이고, 아래에서 확인할 수 있듯이 함수에서 사용되는 세부적인 파라미터에도 차이가 있다. 12rpart(formula, data, weights, subset, na.action = na.rpart, method, model = FALSE, x = FALSE, y = TRUE, parms, control, cost, …) 12345tree(formula, data, weights, subset, na.action = na.pass, control = tree.control(nobs, ...), method = &quot;recursive.partition&quot;, split = c(&quot;deviance&quot;, &quot;gini&quot;), model = FALSE, x = FALSE, y = TRUE, wts = TRUE, ...) 하지만 머리 아파하기에는 아직 이르다. 각각에 모형에 Cross Validation을 적용하고 싶다면 문제는 더 심각해진다. tree 패키지에서 CV를 지원하는 함수는 cv.tree()이고, 이에 대응되는 rpart 패키지의 함수는 print.cp()로, 통일성이라고는 찾아볼 수가 없기 때문이다. 긍정적인 마인드를 가져야 행복해진다고 하니, 함수명과 파라미터 쯤이야 알고리즘별로 정리해놓거나 그냥 외워서 해결할 수 있다고 생각해보자. 그렇다고해도 아직 문제는 남아있다. 각각의 함수에 구현되어 있는 CV는 완전히 동일할까? 이 질문에 대한 나의 답변은 ‘사실 나도 잘 모르겠다’이다. 두 패키지의 함수를 모두 뜯어보기 전까지는 누구도 자신있게 그렇다고 말할 수 없을 것이다. 가장 공포스러운 사실은 Decision Tree만 살펴보아도 이렇게 다른데, 우리가 이용하는 머신러닝 알고리즘은 적게는 수십개에서 많게는 수백개에 이른다는 것이다. 2. 옆 동네 이야기 ㅡ Python과 Scikit-learn잠깐 옆 동네 Python의 사정을 한 번 살펴보자. 오픈 소스라는 특징을 갖는 것은 Python도 마찬가지지만, Python에서의 머신러닝 알고리즘들의 상황은 훨씬 더 깔끔하게 정리되어 있는 것처럼 보인다. Google의 Summer of Code Project(GSoC)로부터 시작된 Scikit-learn이라는 라이브러리가 있기 때문이다. Python에서 구현되는 머신러닝의 사실상의 표준이라고 할 수 있는 Scikit-learn은 머신러닝 모델링 프로세스에서 이용할 수 있는 일관된 인터페이스(Consistent Interface)를 제공한다. Scikit-learn에는 물론 많은 장점들이 있지만, R을 주로 이용하던 내가 해당 라이브러리를 처음 접하면서 가장 먼저 느꼈던 매력 포인트는 모든 모형에 대해 깔끔하게 통일된(일관된) 인터페이스를 제공한다는 점이었다. R에서 패키지마다 모델을 fitting하는 함수가 달랐던 것을 다시 한 번 떠올려보자. 이와는 반대로, Scikit-learn에서는 내가 이용하는 알고리즘에 관계 없이, 모형을 fitting하는 메서드는 항상 fit()이고, 해당 모형을 이용해서 예측을 수행하는 메서드는 항상 predict()이다. 내가 이용하는 모형이 Generalized Linear Regression이든, Support Vector Machine이든, Random Forest이든 상관없다. 단순히 원하는 모형의 객체를 생성하고, fit() 메서드를 실행함으로써 해당 모형을 training(fitting) 시킬 수 있다. 사실 공통적인 기능을 가진 객체들을 구현하기 위해 하나의 클래스를 정의하고, 해당 기능을 동일한 메서드를 통해 제공하는 것은 OOP(Object Oriented Programming)가 지향하는 기본적인 방향성이다. 이러한 맥락에서 생각해보면 Scikit-learn은 OOP의 기본적인 방향성에 잘 부합하는 형태로 설계된 라이브러리라고 이해할 수도 있겠다. 3. 모델링 프로세스모형에 관계없이 공통적인 기능이 요구된다는 이야기는, 모델링 과정에 일정한 프로세스가 존재한다는 의미이기도 하다. 조금만 생각을 해보면, 대부분의 모델링 프로세스는 세부적으로 이용되는 모형과 관계없이 다음과 같은 과정들로 구성되어 있다는 사실을 쉽게 떠올릴 수 있다. &lt; from caret vignettes &gt; 간단히 요약하자면, 데이터 전처리, 후보 파라미터 설정, Cross-Validation을 통한 최적 파라미터의 설정, 최적 파라미터를 이용한 모형 Fitting(Traning) 정도를 공통적인 과정으로 볼 수 있겠다. 다음으로 자연스럽게 떠오르는 질문은, R에는 이와 같은 프로세스의 인터페이스를 통일된 형태로 제공하는 패키지가 없는지에 대한 것이다. 왜 없겠는가? 사실 없으면 이 글을 시작도 안했겠지. 드디어 오늘의 주인공 caret을 소개할 시간이다. 4. 주인공의 등장 - Package:caretcaret은 Classification And REgression Training의 약자로, 예측 모형을 생성하기 위한 매우 다양한 기능들을 제공하는 패키지이다. 포함하고 있는 대략적인 기능들은 다음과 같다. Visualization (wrapper for lattice) Pre-Processing (one-hot-encoding, centering and scaling, Imputation, etc.) Data Splitting Model Training and Parameter Tuning Measuring Performance Parallel Processing 사실 caret은 Scikit-learn과 완전히 동일한 방식ㅡ모형의 객체를 생성하고 메서드를 이용하는 방식ㅡ으로 작동하지는 않는다. 이는 두 언어의 기본적인 Object Oriented System과 관련이 있는데, R은 Generic-function OO를 기반으로 하고 있는 반면에, Python에서의 OOP는 message passing 방식을 기반으로 하고 있기 때문이다. (물론 엄밀히 말하자면 R에서의 객체지향시스템은 S3, S4, RefClass 구분되고, RefClass는 Message Passing 방식이라고 볼 수도 있다. 그러나 Base R을 포함한 대다수의 패키지들은 S3로 구현되어 있으며, caret 또한 마찬가지이다.) 5. Hands-on caret이 글에서는 caret이 제공하는 기능들 중 Data Splitting, Model Training, Predicting, Measuring Performace에 대해서만 매우 가볍게 소개할 예정이다. 알고리즘에 대한 설명이나 결과 해석을 위한 글은 아니니, 개별적인 패키지를 이용할 경우에 비해 caret을 이용할 경우 어떤 식으로 코드가 통일성을 갖게되는지에 집중해서 살펴보면 좋을 것 같다. (1) 데이터의 분할은 createDataPartition 함수를 이용해서 수행할 수 있다. 1234567library(caret)data(iris)#iris = as_tibble(iris)tr_idx = createDataPartition(iris$Species, p = 0.7, list = F)tr_data = iris[tr_idx, ]te_data = iris[-tr_idx, ] (2) 트레이닝에 이용되는 파라미터들은 trainControl() 함수를 통해 설정할 수 있다. 앞서 이야기 했던 Cross Validation을 위해 다음과 같이 설정하고, 두 개의 모형에서 동일한 형태로 해당 파라미터를 이용해보도록 하자. 123par_control = trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 5) (3-1) 첫번째로 시도해 볼 모형은 Random Forest이다. train()이라는 동일한 함수에, method = “rf”를 지정함으로써 fitting할 수 있다. 12345(fit_rf = train(Species ~ ., data = tr_data, method = &quot;rf&quot;, trControl = par_control, verbose = F)) (3-2) fitting한 모형을 통한 예측은 predict() 함수를 통해 수행할 수 있고, confusionMatrix() 함수는 Classification에 이용되는 다양한 Measure들을 한 눈에 살펴볼 수 있는 기능을 제공한다. 12pred_rf = predict(fit_rf, newdata = te_data)confusionMatrix(pred_rf, te_data$Species) (4-1) 다음으로는 Regularized Discriminant Analysis를 수행해보자. Random Forest와 마찬가지로 train() 함수를 이용하지만, 파라미터 method의 값만 “rda”로 변경해주면 된다. 12345(fit_rda = train(Species ~ ., data = tr_data, method = &quot;rda&quot;, trControl = par_control, verbose = F)) (4-2) RDA모형을 이용한 예측에는 역시 동일하게 predict() 함수를 사용하되, 이용하는 모형만 fit_rf에서 fit_rda로 변경해주면 된다. 12pred_rda = predict(fit_rda, newdata = te_data)confusionMatrix(pred_rda, te_data$Species) 6. 일관된 인터페이스(Consistent Interface)의 장점일관된 인터페이스를 이용하는 것은 다음과 같은 장점들을 지닌다. 각 알고리즘이 어떤 패키지를 필요로 하는지 외울 필요가 없다. names(getModelInfo()) 명령어를 통해 caret내에서 이용할 수 있는 알고리즘의 종류를 확인할 수 있다. 튜닝 파라미터 등의 상세한 정보는 ?caret::train 혹은 이 페이지를 통해 살펴볼 수 있다. 각 라이브러리마다 다르게 구현되어 있는 모델을 생성하고, 파라미터를 튜닝하고, 트레이닝하고, 예측하고, 예측 결과의 성능을 평가하는 명령어를 별도로 외울 필요가 없다. 대부분의 모형에 파라미터만 변경된 비슷한 코드를 이용하게 되기 때문에, 코드 재사용에 있어서 효율적인 측면이 있다. 재사용율이 높아진다는 것은 곧 생산성의 제고와도 직결된다. 여기에 덧붙여서 향상된 가독성(Enhanced Readability)은 덤 정도로 가져가면 되겠다. 전처리부터 모델링까지의 과정의 흐름을 코드 상에서 보다 명확하게 확인할 수 있다. 따라서 집중이 필요한 부분에 보다 집중할 수 있는 환경이 구성된다. 이용할 모형을 결정하고, 지표를 보고 판단하고, 파라미터를 결정하는 방법과 같이 의사결정이 필요한 부분에 보다 집중할 수 있다. 이와 같은 코딩 스타일은 일종의 표준으로 작동할 수 있다. 팀 플레이를 할 경우에 더욱 중요해지는 특성인데, 분석 코드를 작성한 사람에 관계없이 코드의 흐름이 통일성을 갖게 되기 때문에 유지 보수가 용이해지는 특성을 갖는다. 7. 마무리작성하다보니 처음 생각보다는 꽤 긴 글이 되었다. 그럼에도 불구하고 caret 패키지를 소개하는 글 치고는 패키지의 기본 기능에 대한 실습 코드가 매우 빈약하다는 느낌을 지울 수가 없을텐데, 이건 특별한 이유 때문은 아니고 작성자가 글을 쓰다가 지쳐서 그렇다. 그래도 최소한의 예의를 위해 추가적인 정보들을 얻을 수 있는 페이지들을 Reference에 남기니, 관심 있으신 분들은 확인해보시면 좋을 것 같다. 사실 어떤 코딩 스타일을 선택할 것인지에 대한 결정은 전적으로 분석가에게 달려있지만, 이 글을 통해 caret 패키지와 분석 프로세스에서의 표준화된 인터페이스에 대한 매력을 조금이라도 느낄 수 있었다면 더할 나위 없이 기쁘겠다. &lt; References &gt; caret: Intro. Document caret: Vignetts caret: R-bloggers tutorial","link":"/R01/"},{"title":"[Taes&#39; R #2] &#39;좋은 형태의 데이터&#39;와 &#39;Tidiness&#39;에 대하여","text":"1. 지난 이야기지난 글에서는 일관된 인터페이스(Consistent Interface)에 대해 살펴보았다. 오늘의 주제에 대해 이야기 하기에 앞서, 잠시 분석 프로세스를 최대한 멀리서 한 번 바라보도록 하자. R을 이용한 데이터 분석을 최대한 간단하게 나타낸다면, 다음과 같이 ‘데이터’를 분석을 위한 ‘함수’에 넣어서 ‘분석 결과’를 도출하는 과정으로 표현할 수 있을 것 같다. * Analysis Process * 이전 글에서 일관된 인터페이스를 지닌 ‘함수’에 대해 살펴보았다면, 이번 글에서 관심을 갖고 살펴볼 대상은 또 하나의 핵심 요소인 ‘데이터’ 그 자체이다. 2. 내가 생각한 플롯은 이게 아닌데…?간단한 데이터 시각화 예시로부터 이야기를 시작해보자. 우리는 시계열 데이터와 R의 기본 그래픽 함수 plot()을 이용해서 두 가지 플롯을 그려볼 것이다. 하나는 시간의 흐름에 따른 값의 변화를 나타내는 라인 그래프가 될 것이고, 다른 하나는 해당 기간동안의 값의 분포를 보여주는 히스토그램이 될 것이다. 샘플 데이터로는 ggplot2 패키지의 economics 데이터를 이용해보도록 하겠다. 해당 데이터셋은 미국의 경제 관련 지표들에 대한 시계열 데이터를 담고 있고, 우리는 이 중에서 psavert라는 항목만을 이용할 것이다. 12345library(tidyverse)data(economics)head(economics)data_ts = economics %&gt;% pull(pce) plot()함수의 Document를 확인해보면, 다음과 같이 플롯의 타입을 지정해 줄 수 있는 것을 확인할 수 있다. type: what type of plot should be drawn. Possible types are &quot;l&quot; for lines &quot;h&quot; for ‘histogram’ like (or ‘high-density’) vertical lines 우선 라인 그래프 먼저 그려보자. 미리 말해두자면, 이 친구에게는 아무런 문제가 없다. 1plot(x=data_ts, type='l') 다음은 type=’h’ 옵션을 이용한 히스토그램의 차례다. 1plot(x=data_ts, type='h') …? 그냥 위에 있는 그래프 아래 면적 색칠한 것 같은데? 우리가 원했던 히스토그램이 아닌, 주어진 데이터 값을 높이로 하는 vertical bar 플롯이 생성되었다. 사실 type=’h’ 옵션을 주면서 히스토그램을 그리고 싶다면, 원본 데이터가 아니라 원본 데이터의 도수 분포표를 생성해서 입력값으로 넣어줘야 한다. 우리가 원했던 형태의 플롯을 얻기위해서 데이터를 8개의 등구간으로 분할한 도수분포표를 구한 후, 이를 통해 히스토그램을 그리는 코드는 다음과 같다. 1234br = seq(min(data_ts), max(data_ts), length.out=9)freq_table = table(cut(data_ts, breaks=br))names(freq_table) = paste(&quot;≤&quot;, round(br[-1], 0))plot(freq_table, type='h', lwd=60, lend=1) 왜 처음에 예상과 다른 그래프가 그려졌을까? plot() 함수가 요구하는 데이터의 형태(도수분포표)가 내가 생각했던 데이터의 형태(원본 데이터)와 달랐기 때문이다. 지난 글에서와 비슷한 문제가 여기에도 존재한다. ‘우리는 그래프마다 정해져있는 데이터의 포멧을 또 모두 외워야하는가?’에 대한 문제이다. 사실 Plotting을 위한 함수와 데이터를 정의하는 데에는 다음과 같은 두 가지 방법이 있다. 데이터의 표준형을 정해두고 Plotting 함수를 데이터에 맞춰 정의하는 방법 Plotting 함수마다 다르게 정의된 데이터 형태에 맞추어 데이터 포멧을 변경하는 방법 두 방법 중 어떤 방법이 더 효율적일까? 복잡하게 따져볼 것도 없이 전자의 방법이 훨씬 더 효율적이다. 데이터 분석 과정에서 대부분이 시간이 데이터 처리에 소요된다는 것은 널리 알려진 사실이다. 그만큼 데이터의 형태를 바꾸는 것은 상당한 작업과 시간이 소요되는 작업이다. 따라서 데이터를 표준적인 형태로 한 번만 변형해놓고, 해당 데이터셋을 계속 이용할 수 있도록 함수를 정의하는 것이 훨씬 더 효율적인 방법이라고 볼 수 있다. 이러한 이유로 전자의 방법을 따르기로 했다면, 자연스럽게 따라오는 질문이 하나 있다. 데이터의 어떤 형태가 좋은 형태인가? 어떤 데이터 포멧을 표준형으로 정해야하나? R 생태계의 절대자Hadley Wickham 교수는 이러한 질문에 대한 답변으로 Tidy Data라는 개념을 제안한다. 이 글에서는 Tidy Data의 개념을 포함하여 다음과 같은 총 세 가지 관점에서 데이터의 형태에 대해 이야기 해 볼 예정이다. Raw Data vs Summary Data (Full Data vs Aggregated Data) Long Form vs Wide Form Tidy Data vs Messy Data 3.데이터의 형태를 바라보는 다양한 관점(1) Raw Data vs Summary Data (Full Data vs Aggregated Data)앞서 economics 데이터셋에서 살펴보았던 원본 데이터와 도수분포표의 관계가 바로 이 첫번째 경우에 해당한다. 이 둘을 구분하는 기준은 ‘데이터 형태 변화에 따른 정보량의 감소 여부’이다. 다르게 표현하자면 ‘데이터의 형태 변환이 Invertible한가?’라고 이야기 할 수도 있겠다. data_ts가 주어지면, 라인 그래프와 (데이터 변환이 필요하기는 하지만) 히스토그램을 모두 생성할 수 있다. 그러나 반대로 freq_table만 주어지는 경우, 히스토그램은 그릴 수 있지만 라인 그래프는 생성할 수가 없다. freq_table은 data_ts가 Summary된 형태의 데이터이고, Sumamry 과정에서 일부 정보가 소실되었기 때문이다. 이러한 형태의 데이터 변환(연산)은 굉장히 쉽게 찾아볼 수 있는데, 대표적으로 우리가 매우 흔하게 접하는 평균을 구하는 연산 또한 그 중 하나이다. 이를테면 어느 회사의 모든 구성원의 연봉 정보를 알 수 있다면 평균 연봉을 구할 수 있지만, 평균 연봉을 안다고 해서 개인별 연봉 정보를 다시 복원할 수는 없다. 이는 전체 데이터인 ‘개별 연봉 정보’가 대표값인 ‘평균 연봉’으로 집계/요약(Aggregation/Summary)되면서 정보량이 감소했기 때문이다. 또 다른 대표적인 예로는 SQL에서의 group by aggregation을 생각해 볼 수도 있겠다. 다시 원래의 주제로 돌아와서, 어떤 형태가 표준형에 적합한지에 대해 이야기해보자면 Sumamry Data로는 표현할 수 있는 정보에 한계가 있기 때문에 Full Data가 상대적으로 적합한 형태라고 볼 수 있겠다. (2) Long Form vs Wide Form앞서 살펴본 (1)번의 경우는 데이터 형태에 따라 정보량에 차이가 발생하게 되는 구분이었다. 이와 달리, 정보량 자체에는 차이가 없더라도 데이터의 형태는 다를 수 있다. 두번째로는 이러한 케이스에 해당하는 Long Form과 Wide Form에 대해서 살펴보도록 하자. Long Form과 Wide Form은 각각 다음과 같은 형태를 의미한다. - Wide Form: 서로 다른 속성을 나타내는 값들이 각각 하나의 컬럼을 이루는 형태 - Long Form: 속성의 종류와 해당 속성의 값을 나타내는 컬럼이 각각 하나씩 존재하는 형태 설명만으로는 충분히 와닿지 않을 수 있으니 데이터 샘플을 통해 형태를 확인해보도록 하자. 예시에는 앞서 활용했던 economics 데이터셋을 그대로 이용하도록 하겠다. 12economics_row3 = economics %&gt;% head(3)economics_row3 A tibble: 3 × 6 datepcepoppsavertuempmedunemploy &lt;date&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; 1967-07-01506.719871212.64.52944 1967-08-01509.819891112.64.72945 1967-09-01515.619911311.94.62958 편의를 위해 economics에서 가장 상위의 3개 행만 뽑아서 살펴보면, 해당 데이터는 date라는 컬럼을 기준으로 해당 일자의 pce, pop, psavert, unempmed, unemploy 라는 5가지 속성을 담고있는 데이터라는 것을 알 수 있다. 각 행을 유니크하게 식별할 수 있는 Unique Key는 Date이며, 5개의 속성은 각각의 컬럼으로 표현되고 있으므로 이 데이터는 Wide Form이라고 볼 수 있다. 이 데이터를 long form으로 변환하면 다음과 같은 형태가 된다. 12economics_row3 %&gt;% pivot_longer(cols=c(pce, pop, psavert, uempmed, unemploy)) A tibble: 15 × 3 datenamevalue &lt;date&gt;&lt;chr&gt;&lt;dbl&gt; 1967-07-01pce 506.7 1967-07-01pop 198712.0 1967-07-01psavert 12.6 1967-07-01uempmed 4.5 1967-07-01unemploy 2944.0 1967-08-01pce 509.8 1967-08-01pop 198911.0 1967-08-01psavert 12.6 1967-08-01uempmed 4.7 1967-08-01unemploy 2945.0 1967-09-01pce 515.6 1967-09-01pop 199113.0 1967-09-01psavert 11.9 1967-09-01uempmed 4.6 1967-09-01unemploy 2958.0 Wide Form에서 하나의 행으로 표현되던 1967년 7월 1일의 데이터는 Long Form에서는 총 5개의 행으로 변환되어 표현되고 있다. 5개의 컬럼으로 표현되던 각각의 속성이, 속성의 이름과 값을 담고있는 5개의 행으로 변환되었으며, 이에 따라 전체 행의 개수는 기존의 3개에서 3(기존 행)*5(속성의 수)=15개로 증가한 것을 살펴볼 수 있다. 하나의 행이 여러 개의 행으로 분리되어 표현되었므로 더 이상 date 컬럼만으로는 각 행을 유니크하게 식별할 수 없으며, Unique Key가 date + name으로 변경된 것 또한 확인할 수 있다. 이미 눈치챈 사람들도 있겠지만, 데이터의 Long/Wide Form 전환은 우리에게 엑셀 혹은 SQL을 통해 이미 친숙한 개념인 Pivot과 매우 연관이 있다. 다시 말해, 우리는 Pivot을 통해 Long Form의 데이터를 데이터를 Wide Form으로, 혹은 그 반대로 변환할 수 있다. 이번 절에서 우리는 데이터의 Long/Wide Form이란 무엇이고, 각 형태 사이의 변환이 어떤 의미를 지니는지에 대해 살펴보았다. 어떤 형태의 데이터를 의미하는 것인지는 알겠는데, 도대체 이런 형태 변환을 왜 하는지에 대한 의문이 드는 것은 어쩌면 자연스러운 일일 것이다. 이러한 의문을 해소하고 조금 더 실용적인 Long/Wide Form의 변환 예시를 확인하고 싶다면 다음 글을 살펴보도록 하자. (3) Tidy Data vs Messy Data앞서 언급했듯이 Tidy Data는 Hadley Wickham 교수가 2014년 8월에 Journal of Statistical Software에 기고한 그의 논문에서 제안한 개념으로, 다음과 같은 세 가지 조건을 만족하는 데이터셋을 의미한다. 각 변수는 하나의 열(column)을 구성한다. (Each variable forms a column.) 각 관측치는 하나의 행(row)를 구성한다. (Each observation forms a row.) 각 관측 기준의 데이터는 하나의 테이블을 구성한다. (Each type of observational unit forms a table.) 정의 자체는 어렵지 않지만, 왜 이러한 조건들이 좋은 데이터의 기준이 되는지는 잘 와닿지 않을 수도 있다. 만약 데이터베이스에 대한 약간의 지식이 있다면 Tidy Data의 개념을 가장 쉽게 이해할 수 있는 방법은 이를 정규형(Codd’s Normal Form)과 연관지어 생각해보는 것이다. 결론부터 이야기하자면 Tidy Data는 데이터베이스에서의 ‘제3정규형’을 의미한다. 두 개념이 어떻게 연관되는지가 궁금하다면, 다음 페이지가 Tidy Data와 제 3정규형의 동치에 대해 간략한 내용을 담고 있으므로 도움이 될 것으로 보인다. This is Codd’s 3rd normal form (Codd 1990), but with the constraints framed in statistical language, and the focus put on a single dataset rather than the many connected datasets common in relational databases. Messy data is any other arrangement of the data. 데이터 정규화의 정의나 필요성, 그리고 그 유용함에 대해서는 이미 잘 정리되어 있는 글들이 많으니 설명을 줄이고자 한다. 다만 여기에서 이야기하고자 하는 Tidiness의 개념은, 데이터베이스에서 추구해온 Efficiency와는 다소 다르다는 점에 유의할 필요가 있다. Tidy Data는 다소간의 중복 등을 포함하더라도 최대한 정보량을 보존하면서, 다른 형태로의 규칙적인 전환이 쉬운 데이터이며, 효율적인 저장과 처리보다는 분석 목적의 활용에 보다 초점이 맞춰져있는 데이터의 형태라고 이해하면 될 것 같다. 4. 마치면서데이터 과학과 관련된 공부를 하다보면, Tidy Data와 Codd’s Normal Form처럼 서로 다른 두 분야가 영향을 주면서 발전하거나, 독립적으로 발전했음에도 불구하고 결국에는 같은 결론에 도달한 경우를 자주 볼 수 있다. 이를 보며 문득 베이즈 정리가 떠올라 관련된 이야기로 글을 마무리해보고자 한다. 베이지안 추론에 대한 하나의 해석은 다음과 같이 표현된다. $$lim_{Experiences \\to \\infty}(Inference\\ Based\\ on\\ Experiences) = Truth$$ 다시 말해, 베이즈 정리의 간단한 수식에는 우리가 현상에 대한 점점 더 많은 정보(혹은 경험, 혹은 데이터)를 모을수록 진리에 한 걸음 더 가까이 다가갈 수 있다는 베이지안 학파의 수학적, 철학적 관점이 반영되어 있다. 각각 독립적인 분야에서 시작된 연구들이 어느 정도 수준에 이르러서 비슷한 결론으로 수렴한다는 사실은 이러한 해석을 다시 한번 떠올려보게 하며, 매우 흥미로운 관점이 아닐 수 없다.","link":"/R02/"},{"title":"[Taes&#39; R #3] 분석을 위한 데이터셋의 형태 변환과 결합에 대하여","text":"1. 지난 이야기지난 글에서는 데이터가 표현될 수 있는 다양한 형태들에 대해 살펴보았다. 이 글에서는 그 중 하나였던 Wide/Long Form 전환의 필요성에 대해 다룰 예정이다. 원활한 이해를 위해 업무로 수행했던 분석 과제 중 하나를 매우 간소화해서 예제로 제시하고, 이를 함께 해결해보고자 한다. 2. 분석 과제 정의이 글에서 우리는 VOD Contents를 고객에게 제공하는 Provider로서, 고객의 VOD 시청 로그를 이용하여 해당 고객의 다음 달 서비스 이탈 여부를 예측해보고자 한다. 각 고객에 대해 이탈 여부를 예측하고자 하므로, 모형의 학습에 이용할 최종 트레이닝 데이터 세트의 하나의 행에는 하나의 고객이 표현되어야 한다. 다시 말해, 우리의 분석 단위(Unit of Analysis)는 고객 개인이며, 하나의 행으로 표현된 각 고객의 여러 특징 정보들은 각각의 개별적인 컬럼으로 표현되도록 할 것이다. 즉, 우리가 모델링을 위해 이용할 데이터는 최종적으로 다음과 같은 형태가 되어야 한다. USER_ID $Y$ $X_1$ $X_2$ … $X_m$ USER1 YES … … … … USER2 NO … … … … … … … … … … USERn YES … … … … 데이터의 각 행을 유니크하게 식별할 수 있는 Unique Key는 USER_ID라는 사실을 기억해두고 다음으로 넘어가보도록 하자. 3. 가상 데이터의 생성이제 우리가 활용할 가상의 VOD 시청 기록 데이터를 생성해볼 차례이다. 순서대로 유저, 컨텐츠, 유저의 컨텐츠 시청 로그 순으로 정의할 것이며, tidyverse 라이브러리를 이용할 예정이다. 12library(tidyverse)set.seed(19) (1) 유저 데이터슬프게도 우리는 당장 내일 망해도 이상할 게 없을 정도로 소규모의 VOD Contents Provider이다. 따라서 우리의 고객은 단 100명 뿐이고, 샘플 데이터의 간결함을 위해 우리가 고객에 대해 가지고 있는 정보는 고객 개개인을 유일하게 식별할 수 있는 user_id와 성별을 나타내는 gender, 그리고 해당 고객의 서비스 이탈 여부를 나타내는 target 뿐이라고 가정한다. 1234567891011n_user = 100target_v = readLines(url(&quot;https://www.dropbox.com/s/o7dbkdl7wwo0wh3/target.txt?raw=1&quot;))# target_v = sample(c(&quot;CHURN&quot;, &quot;STAY&quot;), n_user, replace=TRUE)users = tibble( user_id = paste0(&quot;user_&quot;, str_pad(1:n_user, 2, &quot;left&quot;, &quot;0&quot;)), gender = factor(sample(c(&quot;F&quot;, &quot;M&quot;), n_user, replace=TRUE)), target = factor(target_v, levels=c(&quot;CHURN&quot;, &quot;STAY&quot;)))head(users) A tibble: 6 × 3 user_idgendertarget &lt;chr&gt;&lt;fct&gt;&lt;fct&gt; user_01FCHURN user_02MSTAY user_03MSTAY user_04FCHURN user_05FSTAY user_06MSTAY (2) 컨텐츠 데이터다음으로는 우리가 보유한 VOD 컨텐츠에 대해 정의해보자. 우리가 서비스하는 컨텐츠는 다음과 같은 특징들을 지니고 있다. 보유한 VOD 컨텐츠의 개수는 총 20개이다. 모든 VOD 컨텐츠는 다음 3개 채널 중 하나에 속한다. ch_a, ch_b, ch_c 모든 VOD 컨텐츠는 다음 4개 장르 중 하나에 속한다. drama, horror, comedy, romance 모든 VOD 컨텐츠는 0, 1000, 5000원 중 하나의 가격에 제공된다. 전체 VOD 마켓의 가격별 컨텐츠 비율은 대략적으로 다음과 같다. 가격 비율 0원 60% 1000원 30% 5000원 10% 합계 100% 1234n_content = 20channels = paste0(&quot;ch_&quot;, letters[1:3])genres = c(&quot;drama&quot;, &quot;horror&quot;, &quot;comedy&quot;, &quot;romance&quot;)prices = c(0, 1000, 5000) 12345678contents = tibble( content_id = paste0(&quot;content_&quot;, 1:n_content), content_channel = channels[ceiling(runif(n_content, 0, length(channels)))], content_genre = genres[ceiling(runif(n_content, 0, length(genres)))], content_price = sample(prices, n_content, prob=c(0.6, 0.3, 0.1), replace=TRUE))head(contents) A tibble: 6 × 4 content_idcontent_channelcontent_genrecontent_price &lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;dbl&gt; content_1ch_cromance 0 content_2ch_bromance 0 content_3ch_acomedy 1000 content_4ch_cromance 0 content_5ch_ahorror 1000 content_6ch_ccomedy 0 우리가 방금 생성한 contents 데이터셋은 content_id 컬럼을 이용하여 각 행을 유일하게 식별할 수 있다는 사실을 쉽게 확인할 수 있다. 다시 말해, contents 데이터셋의 Unique Key는 content_id이다. (3) 시청 로그 데이터마지막으로, 우리의 소중한 고객들이 컨텐츠를 시청한 로그 데이터는 다음과 같은 특징들을 가지고 있다. 보유한 시청 로그는 총 3000개이며, 고객마다 시청한 VOD 컨텐츠 개수는 상이하다. 고객이 컨텐츠를 한 번 시청할 때마다, 각 고객별 시퀀스 번호를 신규 생성하여 하나의 레코드(행)로 기록한다. 컨텐츠 1회 시청 시간은 평균 30분 정도이며, 일반적으로 10분 정도 수준의 편차가 존재한다. 고객이 컨텐츠 시청을 시작한 시간대에 대한 정보를 기록하고 있다. 1234567891011n_log = 3000logs = users[ceiling(runif(n_log, 0, n_user)),&quot;user_id&quot;] %&gt;% group_by(user_id) %&gt;% mutate(seq = row_number()) %&gt;% ungroup() %&gt;% mutate( watching_time = floor(runif(n_log, 0, 24)), watching_duration = ceiling(rnorm(n_log, 30, 10)), content_id = pull(contents[ceiling(runif(n_log, 0, n_content)),&quot;content_id&quot;]) ) %&gt;% arrange(user_id, seq) 1head(logs) A tibble: 6 × 5 user_idseqwatching_timewatching_durationcontent_id &lt;chr&gt;&lt;int&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;chr&gt; user_0112025content_9 user_0122135content_19 user_013 029content_17 user_0141224content_16 user_015 510content_19 user_0162031content_19 logs 데이터셋의 경우, 우선 user_id를 통해 users 데이터셋과, content_id를 통해 contents 데이터셋과 결합할 수 있으리라는 점을 쉽게 살펴볼 수 있다. 다만 users 데이터셋과는 달리 각 행은 user_id 하나만으로는 유일하게 식별되지 않으며, seq 컬럼이 추가된 user_id + seq가 Unique Key가 된다는 사실 또한 확인할 수 있다. 4. 최종 데이터의 형태이제 우리가 어떤 데이터를 보유하고 있는지 모두 살펴보았다. 지금부터는 이 세 개의 데이터를 어떻게 조합해서 모델 학습에 이용할 최종 데이터셋을 생성할 지에 대해 생각해보자. 우선 시청 로그와 컨텐츠 데이터를 먼저 결합해보자. 시청 로그 데이터를 기준으로 우측에 컨텐츠 정보를 추가하는 형태가 될 것이고, 결합을 위한 Join Key는 각 데이터셋의 content_id 컬럼이 될 것이다. 결합이 완료된 데이터셋의 Unique Key는 contents 데이터셋과 동일하게 user_id + seq로 유지된다. 1234logs_with_contents = logs %&gt;% left_join(contents, by = c(&quot;content_id&quot;))head(logs_with_contents) A tibble: 6 × 8 user_idseqwatching_timewatching_durationcontent_idcontent_channelcontent_genrecontent_price &lt;chr&gt;&lt;int&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;dbl&gt; user_0112025content_9 ch_cdrama 0 user_0122135content_19ch_bdrama 0 user_013 029content_17ch_adrama 0 user_0141224content_16ch_ahorror0 user_015 510content_19ch_bdrama 0 user_0162031content_19ch_bdrama 0 이제 남은 문제는 유저 데이터와 시청 로그 데이터를 어떻게 결합할 것인지에 대한 부분이다. 둘 중 어느 데이터가 기준 데이터(Left Join을 가정할 때 Left에 위치하게 될 데이터)가 되어야 할 지 먼저 생각해보자. 앞서 2. 분석 과제 정의에서 우리의 분석 단위는 고객 개인이며, 따라서 학습에 이용할 최종 데이터셋에서 각 고객은 하나의 행으로 표현되어야 한다는 부분을 이미 언급했다. 즉, 최종 데이터셋의 Unique Key는 user_id가 되어야 할 것이다. 만약 단순하게 user_id를 기준으로 두 데이터셋(users와 logs_with_contents)를 결합하면 어떻게 될까? logs_with_contents 데이터의 각 고객은 여러 행에 걸쳐서 표현되고 있으므로, 두 데이터셋을 단순 결합할 경우 결과 데이터셋 또한 한 명의 고객의 데이터가 여러 행에 걸쳐서 나타나게 될 것이다. 다르게 표현하자면, 두 데이터셋의 Unique Key의 관계가 1:N이기 때문에, 1에 해당하는 user_id를 Unique Key로 유지한 상태로 결합하기 위해서는 N에 해당하는 데이터셋의 변환이 필요하다는 이야기이다. 따라서 우리가 원하는 최종 데이터 형태를 얻기 위해서는 users를 기준으로 데이터를 결합하되, N에 해당하는 logs_with_contents의 데이터는 다음의 세 가지 방법 중 하나를 통해 변환되어야만 한다. 다수의 행에서 특정 행을 선택 분석 단위(Unit of Analysis, user_id)를 기준으로 집계 분석 단위보다 세부 단위를 기준으로 집계 후, 결과 행의 개별 컬럼화 이해를 돕기 위해 두 데이터셋에서 user_id가 ‘user_08’에 해당하는 데이터를 예시로 살펴보면서 각 방법에 대해서 살펴보도록 하자. 5. 결합을 위한 데이터의 변환users 데이터셋에서 user_id가 ‘user_08’인 행은 단 하나뿐인 반면, logs_with_contents 데이터셋에서 user_id가 ‘user_08’인 행은 총 29개이다. 지금 우리의 관심사는 이 스물 아홉개의 행을 users의 하나의 행과 결합하기 위해서 어떻게 선택 혹은 요약해서 하나의 행으로 표현할 것인지에 대한 것이다. 12users %&gt;% filter(user_id=='user_08') A tibble: 1 × 3 user_idgendertarget &lt;chr&gt;&lt;fct&gt;&lt;fct&gt; user_08FSTAY 12logs_with_contents %&gt;% filter(user_id=='user_08') A tibble: 29 × 8 user_idseqwatching_timewatching_durationcontent_idcontent_channelcontent_genrecontent_price &lt;chr&gt;&lt;int&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;dbl&gt; user_08 1 730content_12ch_adrama 0 user_08 21029content_2 ch_bromance 0 user_08 31528content_6 ch_ccomedy 0 user_08 4 840content_18ch_chorror 0 user_08 52118content_2 ch_bromance 0 user_08 61922content_18ch_chorror 0 user_08 7 922content_8 ch_acomedy 1000 user_08 81131content_18ch_chorror 0 user_08 9 236content_13ch_acomedy 0 user_0810 518content_2 ch_bromance 0 user_08111739content_3 ch_acomedy 1000 user_0812 341content_7 ch_bromance1000 user_0813 037content_18ch_chorror 0 user_0814 539content_4 ch_cromance 0 user_0815 223content_20ch_bromance 0 user_08162131content_12ch_adrama 0 user_0817 433content_2 ch_bromance 0 user_08182216content_20ch_bromance 0 user_08191736content_9 ch_cdrama 0 user_08201311content_12ch_adrama 0 user_0821 235content_15ch_chorror 1000 user_0822 223content_10ch_cromance 0 user_08231518content_5 ch_ahorror 1000 user_08241346content_20ch_bromance 0 user_08251635content_20ch_bromance 0 user_0826 035content_19ch_bdrama 0 user_0827 950content_15ch_chorror 1000 user_0828 316content_1 ch_cromance 0 user_0829 745content_12ch_adrama 0 (1) 첫 번째 방법: 특정 행의 선택가장 쉽고 간단한 방법은 스물 아홉개의 행 중에서 하나의 행만을 선택해서 결합하는 것이다. 이를테면, 전체 시청 로그 중에서 시청 시간이 가장 긴 행을 대표행으로 선택하거나, 가장 최근의 시청 로그에 해당하는 행을 대표행으로 선택하고 나머지 행을 모두 버리는 방법을 생각해 볼 수 있다. 123# users 데이터셋에서 user_id가 'user_08'인 행만 추출 (row_u8 = users %&gt;% filter(user_id=='user_08')) A tibble: 1 × 3 user_idgendertarget &lt;chr&gt;&lt;fct&gt;&lt;fct&gt; user_08FSTAY 123456789# user_08의 컨텐츠 시청 로그 중, 시청 시간(watching_duration)이 가장 긴 행만 추출(method_1_select1 = logs_with_contents %&gt;% filter(user_id=='user_08') %&gt;% filter(watching_duration==max(watching_duration)) %&gt;% filter(seq==min(seq)))# 결합row_u8 %&gt;% left_join(method_1_select1, by='user_id') A tibble: 1 × 8 user_idseqwatching_timewatching_durationcontent_idcontent_channelcontent_genrecontent_price &lt;chr&gt;&lt;int&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;dbl&gt; user_0827950content_15ch_chorror1000 A tibble: 1 × 10 user_idgendertargetseqwatching_timewatching_durationcontent_idcontent_channelcontent_genrecontent_price &lt;chr&gt;&lt;fct&gt;&lt;fct&gt;&lt;int&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;dbl&gt; user_08FSTAY27950content_15ch_chorror1000 12345678# user_08의 컨텐츠 시청 로그 중, 가장 최근에 시청한 컨텐츠에 대한 행만 추출(method_1_select2 = logs_with_contents %&gt;% filter(user_id=='user_08') %&gt;% filter(seq==max(seq)))# 결합row_u8 %&gt;% left_join(method_1_select2, by='user_id') A tibble: 1 × 8 user_idseqwatching_timewatching_durationcontent_idcontent_channelcontent_genrecontent_price &lt;chr&gt;&lt;int&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;dbl&gt; user_0829745content_12ch_adrama0 A tibble: 1 × 10 user_idgendertargetseqwatching_timewatching_durationcontent_idcontent_channelcontent_genrecontent_price &lt;chr&gt;&lt;fct&gt;&lt;fct&gt;&lt;int&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;dbl&gt; user_08FSTAY29745content_12ch_adrama0 사실 이러한 첫번째 방법은 지금 우리가 가진 데이터에서는 크게 의미가 없는 방법일 수도 있다. 하지만 예를 들어 한명의 고객이 여러 개의 계정/프로필을 가질 수 있고, 그 중에서 대표 계정/프로필에 대한 데이터만 선택해서 결합할 필요가 있다거나 하는 경우에는 매우 유용하게 활용될 수 있는 방법이다. (2) 두 번째 방법: 분석 단위를 기준으로 집계두 번째로는 최종 분석 단위 user_id를 기준으로 데이터를 집계(Aggregation/Summary)하는 변환에 대해서 살펴볼 것이다. 예를 들어, logs_with_contents 데이터로부터 user_id별로 총 시청 시간을 집계한 후에 users 데이터셋과 결합해보도록 하자. 우선 user_08의 29개의 시청 로그는 다음과 같이 하나의 행으로 요약될 수 있다. 1234(total_watching_duration_u8 = logs_with_contents %&gt;% filter(user_id=='user_08') %&gt;% group_by(user_id) %&gt;% summarise(total_watching_duration = sum(watching_duration))) A tibble: 1 × 2 user_idtotal_watching_duration &lt;chr&gt;&lt;dbl&gt; user_08883 users의 user_08에 대한 행도 하나이므로, 요약된 데이터와 다음과 같이 쉽게 결합할 수 있다. 12row_u8 %&gt;% left_join(total_watching_duration_u8, by='user_id') A tibble: 1 × 4 user_idgendertargettotal_watching_duration &lt;chr&gt;&lt;fct&gt;&lt;fct&gt;&lt;dbl&gt; user_08FSTAY883 ‘user_08’만을 필터링하지 않은 전체 고객에 대한 집계와 결합 또한 다음 코드를 통해 어렵지 않게 수행할 수 있다. 123456# 각 고객별 총 시청 시간 집계total_watching_duration = logs_with_contents %&gt;% group_by(user_id) %&gt;% summarise(total_watching_duration = sum(watching_duration))head(total_watching_duration) A tibble: 6 × 2 user_idtotal_watching_duration &lt;chr&gt;&lt;dbl&gt; user_01 684 user_02 980 user_031100 user_041155 user_05 810 user_06 704 최종 분석 단위를 기준으로 집계된 데이터의 Unique Key는 당연히 최종 분석 단위 user_id 가 된다. 따라서 users 데이터셋과 total_watching_duration 데이터셋의 Unique Key는 모두 user_id로 1:1 관계이므로, 다음과 같이 쉽게 결합할 수 있다. 1234# 결합users %&gt;% left_join(total_watching_duration, by='user_id') %&gt;% head() A tibble: 6 × 4 user_idgendertargettotal_watching_duration &lt;chr&gt;&lt;fct&gt;&lt;fct&gt;&lt;dbl&gt; user_01FCHURN 684 user_02MSTAY 980 user_03MSTAY 1100 user_04FCHURN1155 user_05FSTAY 810 user_06MSTAY 704 (3) 세 번째 방법: 분석 단위보다 세부 단위를 기준으로 집계 후, 결과 행의 개별 컬럼화우리는 앞서 살펴본 것처럼 각 고객별 총 시청 시간을 집계해서 변수로 활용할 수도 있지만, 채널별 시청 시간을 집계해서 조금 더 세부적인 정보를 나타내는 변수로 활용하고 싶을 수도 있다. 이 때 집계의 기준 그룹은 기존과 달리 user_id가 아니라 user_id + content_channel이 되고, 집계 결과는 다음 코드를 통해 확인해 볼 수 있다. 12345watching_duration_by_channel = logs_with_contents %&gt;% group_by(user_id, content_channel) %&gt;% summarise(total_watching_duration = sum(watching_duration))head(watching_duration_by_channel, 12) `summarise()` has grouped output by 'user_id'. You can override using the `.groups` argument. A grouped_df: 12 × 3 user_idcontent_channeltotal_watching_duration &lt;chr&gt;&lt;chr&gt;&lt;dbl&gt; user_01ch_a159 user_01ch_b330 user_01ch_c195 user_02ch_a371 user_02ch_b189 user_02ch_c420 user_03ch_a412 user_03ch_b267 user_03ch_c421 user_04ch_a212 user_04ch_b353 user_04ch_c590 (2)에서와 달리 집계 결과의 Unique Key는 user_id + content_channel로 우리의 분석 단위 user_id와 일치하지 않는다. 다시 말해, 각 유저별로 3개의 채널에 대한 행을 가지고 있기 때문에 users와 watching_duration_by_channel의 관계는 여전히 1:n이며, 우리는 각 유저의 채널a, 채널b, 채널c 시청 시간을 별도의 컬럼들로 분리시킴으로써 하나의 레코드(행)에 하나의 유저에 대한 정보만 표현될 수 있도록 형태를 변경해볼 것이다. 즉, 현재 1차로 집계한 데이터를 보다 Wide한 형태로 변환할 것이다. 12345678watching_duration_by_channel_wider = watching_duration_by_channel %&gt;% pivot_wider( names_from=c(content_channel), names_prefix=&quot;watching_duration_&quot;, values_from = total_watching_duration )head(watching_duration_by_channel_wider, 4) A grouped_df: 4 × 4 user_idwatching_duration_ch_awatching_duration_ch_bwatching_duration_ch_c &lt;chr&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; user_01159330195 user_02371189420 user_03412267421 user_04212353590 이전 집계 결과에서는 watching_duration이라는 하나의 컬럼에서 3개 행으로 표현되던 채널별 시청 시간이, 각 채널별로 별도 컬럼으로 표현되도록 형태가 변환된 것을 확인해 볼 수 있다. (3 rows * 1 column)의 형태가 (1 row * 3 columns)으로 변환되면서, 각 user_id의 시청 정보는 하나의 행에 표현되고 있다. 따라서 변환된 집계 결과의 Unique Key는 user_id가 되며, 이전과 마찬가지로 users 데이터셋과 쉽게 결합할 수 있게 되었다. 123users %&gt;% left_join(watching_duration_by_channel_wider, by=&quot;user_id&quot;) %&gt;% head(4) A tibble: 4 × 6 user_idgendertargetwatching_duration_ch_awatching_duration_ch_bwatching_duration_ch_c &lt;chr&gt;&lt;fct&gt;&lt;fct&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; user_01FCHURN159330195 user_02MSTAY 371189420 user_03MSTAY 412267421 user_04FCHURN212353590 마지막으로 조금만 더 깊이 들어가보자. 채널별로 시청 시간만을 집계했던 조금 전과는 달리, 그룹별로 여러 개의 요약값을 집계하면 데이터는 어떤 형태가 될까? 예를 들어, 장르별로 ‘시청 시간’, ‘시청 횟수’, ‘결제액 합계’, ‘최고가 결제액’의 네 가지 수치를 동시에 집계할 경우의 데이터 형태에 대해서 살펴보도록 하자. 앞서 살펴보았던 케이스와의 차이점은 다음과 같이 정리해 볼 수 있다. 유저별 채널별 시청 시간 합계 집계 그룹: user_id + content_channel 요약값: 시청 시간 합계 sum(watching_duration) 유저별 장르별 시청 시간 합계, 시청 횟수, 결제액 합계, 최고가 결제액 집계 그룹: user_id + conetent_genre 요약값: 시청 시간 합계 sum(watching_duration) 시청 횟수 count(content_id) 결제액 합계 sum(content_price) 최고가 결제액 max(content_price 기존과 동일한 방법으로 logs_with_contents 데이터셋을 집계한 결과는 다음과 같다. 12345678910agg_by_genre = logs_with_contents %&gt;% group_by(user_id, content_genre) %&gt;% summarise( duration_sum = sum(watching_duration), watching_cnt = n(), price_sum = sum(content_price), price_max = max(content_price) )head(agg_by_genre, 12) `summarise()` has grouped output by 'user_id'. You can override using the `.groups` argument. A grouped_df: 12 × 6 user_idcontent_genreduration_sumwatching_cntprice_sumprice_max &lt;chr&gt;&lt;chr&gt;&lt;dbl&gt;&lt;int&gt;&lt;dbl&gt;&lt;dbl&gt; user_01comedy 30 110001000 user_01drama 34111 0 0 user_01horror 70 320001000 user_01romance2431050001000 user_02comedy 217 830001000 user_02drama 126 4 0 0 user_02horror 276 940001000 user_02romance3611350001000 user_03comedy 3601020001000 user_03drama 141 5 0 0 user_03horror 3341060001000 user_03romance2651020001000 출력된 결과와 이전 집계 결과와의 차이점에 대해서 하나씩 살펴보도록 하자. 우선, 컨텐츠의 채널이 ch_a, ch_b, ch_c의 3개 값을 가질 수 있으므로 각 고객의 시청 로그가 3개 행에 걸쳐서 표현되었던 이전 집계 결과와 달리, 장르는 comedy, drama, horror, romance 4개의 값을 가질 수 있으므로 각 고객의 시청로그가 4개 행에 걸쳐서 표현되고 있다는 사실을 쉽게 확인해 볼 수 있다. 다음으로는, 집계된 수치에 해당하는 값이 total_watching_duration 하나 뿐이었던 이전과 달리, 이번 집계 결과에서는 duration_sum, watching_cnt, price_sum, price_max의 네 개의 수치가 집계되었다는 점 또한 확인할 수 있다. 데이터의 Shape 관점에서는 다음과 같이 정리해 볼 수 있다. Type Form Row Column Total 채널별 집계 결과 Long 3 1 3 형태 변환 후 Wide 1 3 3 상단의 표는 우리가 Long Form의 형태로 표현된 채널별 집계 결과의 각 행을 별도의 컬럼으로 표현함으로써 Wide Form으로 변환할 수 있었다는 것을 의미한다. 한편, 장르별 집계 결과와 우리가 형태 변환을 통해서 얻고자 하는 데이터의 Shape은 다음과 같이 정리해 볼 수 있을 것이다. Type Form Row Column Total 장르별 집계 결과 - 4 4 16 형태 변환 후 Wide 1 16 16 이전과 달리 집계 결과가 완전한 Long Form이 아니기 때문에, 이전과 동일한 방법으로 바로 Wide Form으로 변환하기 보다는, 일차적으로 Long Form으로 변환 후에 Wide Form으로 재변환하는 방식으로 데이터의 형태를 바꿔볼 것이다. 이를 정리하자면 다음과 같다. Type Form Row Column Total 장르별 집계 결과 - 4 4 16 1차 변환 후 Long 16 1 16 2차 변환 후 Wide 1 16 16 Long Form으로 데이터를 표현하기 위한 1차 변환은 다음과 같이 수행할 수 있다. 한명의 고객의 데이터가 최대 16개의 행에 걸쳐서 표현된다는 점에 유의해서 살펴보도록 하자. 1234agg_by_genre_longer = agg_by_genre %&gt;% pivot_longer(cols = c(duration_sum, price_sum, price_max, watching_cnt))agg_by_genre_longer %&gt;% head(48) A grouped_df: 48 × 4 user_idcontent_genrenamevalue &lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;dbl&gt; user_01comedy duration_sum 30 user_01comedy price_sum 1000 user_01comedy price_max 1000 user_01comedy watching_cnt 1 user_01drama duration_sum 341 user_01drama price_sum 0 user_01drama price_max 0 user_01drama watching_cnt 11 user_01horror duration_sum 70 user_01horror price_sum 2000 user_01horror price_max 1000 user_01horror watching_cnt 3 user_01romanceduration_sum 243 user_01romanceprice_sum 5000 user_01romanceprice_max 1000 user_01romancewatching_cnt 10 user_02comedy duration_sum 217 user_02comedy price_sum 3000 user_02comedy price_max 1000 user_02comedy watching_cnt 8 user_02drama duration_sum 126 user_02drama price_sum 0 user_02drama price_max 0 user_02drama watching_cnt 4 user_02horror duration_sum 276 user_02horror price_sum 4000 user_02horror price_max 1000 user_02horror watching_cnt 9 user_02romanceduration_sum 361 user_02romanceprice_sum 5000 user_02romanceprice_max 1000 user_02romancewatching_cnt 13 user_03comedy duration_sum 360 user_03comedy price_sum 2000 user_03comedy price_max 1000 user_03comedy watching_cnt 10 user_03drama duration_sum 141 user_03drama price_sum 0 user_03drama price_max 0 user_03drama watching_cnt 5 user_03horror duration_sum 334 user_03horror price_sum 6000 user_03horror price_max 1000 user_03horror watching_cnt 10 user_03romanceduration_sum 265 user_03romanceprice_sum 2000 user_03romanceprice_max 1000 user_03romancewatching_cnt 10 Long Form으로 1차 변환된 집계 결과를 다시 Wide Form으로 최종 변환하는 과정은 다음 코드를 통해서 수행할 수 있다. 1234agg_by_genre_wider = agg_by_genre_longer %&gt;% pivot_wider(names_from = c(content_genre, name))head(agg_by_genre_wider, 3) A grouped_df: 3 × 17 user_idcomedy_duration_sumcomedy_price_sumcomedy_price_maxcomedy_watching_cntdrama_duration_sumdrama_price_sumdrama_price_maxdrama_watching_cnthorror_duration_sumhorror_price_sumhorror_price_maxhorror_watching_cntromance_duration_sumromance_price_sumromance_price_maxromance_watching_cnt &lt;chr&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; user_01 3010001000 13410011 7020001000 32435000100010 user_0221730001000 812600 427640001000 93615000100013 user_03360200010001014100 533460001000102652000100010 드디어 우리가 원했던대로 각 고객의 정보가 하나의 행에 표현될 수 있도록 데이터의 형태가 변환되었다. 최종 변환된 집계 결과의 Unique Key는 user_id가 되며, 앞서 살펴보았던 케이스들과 마찬가지로 users 데이터셋과 쉽게 결합함으로써 분석을 위한 최종 데이터셋을 생성할 수 있다. 1234data_for_analysis = users %&gt;% left_join(agg_by_genre_wider, by='user_id')head(data_for_analysis, 10) A tibble: 10 × 19 user_idgendertargetcomedy_duration_sumcomedy_price_sumcomedy_price_maxcomedy_watching_cntdrama_duration_sumdrama_price_sumdrama_price_maxdrama_watching_cnthorror_duration_sumhorror_price_sumhorror_price_maxhorror_watching_cntromance_duration_sumromance_price_sumromance_price_maxromance_watching_cnt &lt;chr&gt;&lt;fct&gt;&lt;fct&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; user_01FCHURN 3010001000 13410011 7020001000 32435000100010 user_02MSTAY 21730001000 812600 427640001000 93615000100013 user_03MSTAY 360200010001014100 533460001000102652000100010 user_04FCHURN27820001000 926100 914530001000 54714000100016 user_05FSTAY 318500010001012600 418930001000 617710001000 6 user_06MSTAY 15030001000 4 7700 228340001000 919430001000 8 user_07FCHURN305400010001017100 522920001000 84324000100013 user_08FSTAY 12520001000 418800 623330001000 73371000100012 user_09FSTAY 20220001000 620000 626760001000 83894000100014 user_10FSTAY 334500010001015400 519830001000 63405000100011 물론 이러한 형태의 데이터 형태 변환이 익숙하다면, 초기 집계 결과의 형태에서 Long Form으로의 중간 전환을 건너뛰고 바로 Wide Form으로 전환하는 것도 가능하다. 이를 위한 코드는 다음과 같으며, 함수의 각 파라미터의 의미에 대해서는 글을 읽는 우리 모두가 형태 변환에 보다 익숙해지기를 바라는 마음으로 직접 고민해 볼 부분으로 남겨놓는다. 123456agg_by_genre %&gt;% pivot_wider( names_from=c(content_genre), values_from = c(duration_sum, price_sum, price_max, watching_cnt) ) %&gt;% head(10) A grouped_df: 10 × 17 user_idduration_sum_comedyduration_sum_dramaduration_sum_horrorduration_sum_romanceprice_sum_comedyprice_sum_dramaprice_sum_horrorprice_sum_romanceprice_max_comedyprice_max_dramaprice_max_horrorprice_max_romancewatching_cnt_comedywatching_cnt_dramawatching_cnt_horrorwatching_cnt_romance &lt;chr&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;int&gt;&lt;int&gt;&lt;int&gt;&lt;int&gt; user_01 30341 7024310000200050001000010001000 111 310 user_0221712627636130000400050001000010001000 8 4 913 user_033601413342652000060002000100001000100010 51010 user_0427826114547120000300040001000010001000 9 9 516 user_053181261891775000030001000100001000100010 4 6 6 user_06150 7728319430000400030001000010001000 4 2 9 8 user_073051712294324000020004000100001000100010 5 813 user_0812518823333720000300010001000010001000 4 6 712 user_0920220026738920000600040001000010001000 6 6 814 user_103341541983405000030005000100001000100010 5 611 6. 간단한 모델링다음은 생성한 데이터를 이용한 간단한 모델링과 결과 확인을 위한 샘플 코드이다. 랜덤하게 제너레이팅한 부분이 많은 데이터셋이라서 성능이 만족할만한 수준으로 나오지는 않지만, 모델링을 위한 글이 아니므로 세부적인 내용에 대해서는 다루지 않을 예정이다. 사실 모델링에 대한 내용을 제외해도 이미 한번에 읽기 부담스러울만큼 충분히 긴 글이기도 하다. 12345for (pkg in c(&quot;caret&quot;, &quot;e1071&quot;, &quot;randomForest&quot;)) if (!pkg %in% installed.packages()[,1]) install.packages(pkg)library(caret) Loading required package: lattice Attaching package: ‘caret’ The following object is masked from ‘package:purrr’: lift 123# Imputation for Missing Valuesdata_imp = data_for_analysis %&gt;% mutate_if(is.numeric, ~replace(., is.na(.), 0)) 12345idx_tr = createDataPartition(pull(data_imp, target), p = 0.7, list=FALSE)data_tr = select(data_imp[idx_tr, ], -user_id)data_te = select(data_imp[-idx_tr, ], -user_id)actual = pull(data_te, target) 123fit_glm = glm(target ~ ., data=data_tr, family=binomial)pred_prob_glm = predict(fit_glm, newdata=data_te, type='response')pred_glm = as.factor(ifelse(pred_prob_glm &gt; 0.5, levels(actual)[2], levels(actual)[1])) 123fit_rf = train(target ~ ., data = data_tr, method = &quot;rf&quot;, verbose = F)pred_prob_rf = predict(fit_rf, newdata=data_te, type='prob')pred_rf = as.factor(ifelse(pred_prob_rf[,2] &gt; 0.5, &quot;STAY&quot;, &quot;CHURN&quot;)) 1confusionMatrix(pred_glm, actual) Confusion Matrix and Statistics Reference Prediction CHURN STAY CHURN 5 3 STAY 6 15 Accuracy : 0.6897 95% CI : (0.4917, 0.8472) No Information Rate : 0.6207 P-Value [Acc &gt; NIR] : 0.287 Kappa : 0.304 Mcnemar's Test P-Value : 0.505 Sensitivity : 0.4545 Specificity : 0.8333 Pos Pred Value : 0.6250 Neg Pred Value : 0.7143 Prevalence : 0.3793 Detection Rate : 0.1724 Detection Prevalence : 0.2759 Balanced Accuracy : 0.6439 'Positive' Class : CHURN 1confusionMatrix(pred_rf, actual) Confusion Matrix and Statistics Reference Prediction CHURN STAY CHURN 6 1 STAY 5 17 Accuracy : 0.7931 95% CI : (0.6028, 0.9201) No Information Rate : 0.6207 P-Value [Acc &gt; NIR] : 0.03859 Kappa : 0.5272 Mcnemar's Test P-Value : 0.22067 Sensitivity : 0.5455 Specificity : 0.9444 Pos Pred Value : 0.8571 Neg Pred Value : 0.7727 Prevalence : 0.3793 Detection Rate : 0.2069 Detection Prevalence : 0.2414 Balanced Accuracy : 0.7449 'Positive' Class : CHURN 7. Wrap-Up이번 글에서는 원장성 데이터(마스터)와 이력성 데이터(히스토리)를 가지고 있을 때, 분석을 위한 최종 테이터셋을 생성하는 과정에 대해서 살펴보았다. 그 중에서도 특히 원장성 데이터와 결합하기 위해 이력성 데이터를 선택/집계하는 세 가지 방법에 대해 다루었으며, 각 방법을 데이터셋 간의 관계와 데이터의 형태 변환 측면에서 자세히 살펴보고자 했다. 특히 세 가지 방법 중에서 마지막에 해당하는 방법을 다루면서는, 지난 글에서 살펴보았던 데이터의 Long/Wide Form 변환에 대해 조금 더 자세히 설명하고자 했다. 이번 글을 통해서 지난 글에서 설명이 다소 부족했던 형태 변환의 필요성과 구체적인 변환 방법에 대한 실마리를 얻을 수 있었다면 더할 나위 없는 보람이 되겠다. 적지 않은 내용을 다소 많은 관점에서 다루면서 글이 다소 복잡해진 느낌이 들긴 하지만, 누군가에게는 도움이 될 수 있는 내용이기를 바라면서 글을 마친다.","link":"/R03/"},{"title":"Weekly-NLP, Introduction","text":"공부했던 내용을 정리하고 기록으로 남기고자 하는 목적으로 글을 작성해 보고자 합니다. 이 시리즈는 크게 두 파트로 나뉘어질 예정입니다. 첫번째 파트에서는 NLP Tasks를 모델링하기 위한 기본적인 지식들에 대해서 다루며, 다음과 같은 개념들에 대해 살펴볼 예정입니다. Language Models Word Dependencies and N-gram Model RNN LSTM Sequence to Sequence (Encoder and Decoder) Architecture Attention Transformer blocks and Transformer 두 번째 파트에서는, 앞서 다루었던 접근법들이 다양한 NLP Tasks에 어떻게 적용되는지에 대해서 다룰 예정입니다. 즉, 해당 모형들의 Applications에 집중할 예정이며, 여기에는 다음과 같은 내용들이 포함됩니다. Word Embeddings Machine Translation Pre-trained Language models, e.g., BERT, GPT3 Syntactic Parsing, Semantic Parsing Paraphrasing Question Answering Summarisation Data-to-Text Generation Sentiment Analysis 작성하는 글에서는 엄밀한 증명이나 수학적인 이해보다는, 전반적인 흐름을 살펴보는 데에 집중하고자 합니다. 즉, 이 시리즈는 특정 문제를 해결하는데에 있어서 기존 접근법이 어떤 문제가 있었고, 이를 해결하기 위해서 어떠한 접근방법이 제시되었으며, 새로 제시된 접근법의 한계는 무엇이었는지 등에 초점을 맞추고 있습니다. 마지막으로, 이 시리즈는 에딘버러 대학교(University of Edinburgh)의 다음 세 강의에 기반하고 있다는 사실을 미리 밝힙니다. INFR11125: Accelerated Natural Language Processing INFR11157: Natural Language Understanding, Generation and Machine Translation INFR11145: Text Technologies for Data Science","link":"/Weekly-NLP-Intro/"}],"tags":[{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"WNLP-PART1","slug":"WNLP-PART1","link":"/tags/WNLP-PART1/"},{"name":"Information Theory","slug":"Information-Theory","link":"/tags/Information-Theory/"},{"name":"Others","slug":"Others","link":"/tags/Others/"},{"name":"R","slug":"R","link":"/tags/R/"}],"categories":[{"name":"NLP","slug":"NLP","link":"/categories/NLP/"},{"name":"WeeklyNLP","slug":"WeeklyNLP","link":"/categories/WeeklyNLP/"},{"name":"Information Theory","slug":"Information-Theory","link":"/categories/Information-Theory/"},{"name":"Others","slug":"Others","link":"/categories/Others/"},{"name":"R","slug":"R","link":"/categories/R/"}]}