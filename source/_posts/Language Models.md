---
title: Language Models
date: 2023/02/01
category:
- WeeklyNLP
tag:
- NLP
- WNLP-PART1
toc: true
---

결론부터 이야기하자면, 언어 모델*language model*은 각 문자열에 확률을 부여한 확률 모형*probabilistic model of strings*이고, 언어 모델링*language modelling*은 이러한 확률을 부여하기 위한 과정을 의미합니다. 이 글에서는 어떠한 맥락에서 이러한 정의가 성립하는지 살펴보도록 하겠습니다.

우리가 한국어를 할 줄 안다는 것은 무엇을 의미할까요? 친구가 우리에게 ‘오늘 뭐 해?’라고 물어본 상황을 한 번 가정해보겠습니다. 우리가 질문을 정확히 이해했다면 아마 다음의 (1), (2)번과 같은 대답들을 떠올릴 수 있을 것입니다. 한편, 동일한 질문에 대해서 (3)번이나 (4)번처럼 대답할 일은 별로 없겠죠.

> 1) 오늘은 집에서 좀 쉬려고!
> 2) 응, 너랑은 안놀아.
> 3) 어제 피자 먹었어.
> 4) 나는 요즘 박보영이 좋더라.

이를 확률의 관점에서 이야기 해보자면, 우리가 ‘오늘 뭐 해?’라는 질문을 받았을 때 ’오늘은 집에서 좀 쉬려고!’ 혹은 ‘응, 너랑은 안놀아.’라는 문장을 선택할 확률은 ‘어제 피자 먹었어.’나 ‘나는 요즘 박보영이 좋더라.’라는 문장을 선택할 확률보다 훨씬 더 높아야 한다는 의미입니다. 즉, ‘언어를 이해하고 사용한다’라는 개념을 매우 거칠게 정의하자면, 해당 언어에서 각 문자열들의 발생 확률을 추측하는 과정이라고 볼 수 있습니다. 이러한 맥락에서, 우리는 모든 후보 문자열에 대한 발생 확률을 추정하는 과정을 언어 모델링*language modelling*, 그러한 과정을 통해 생성된 문자열들의 확률 분포를 언어 모형*language model*이라고 부릅니다.

## 확률의 추정 — MLE
앞서 우리는 언어 모델링을 각 문자열에 대한 발생 확률을 추정하는 과정으로 정의했습니다. 그러면 이러한 확률들은 어떻게 추정할 수 있을까요? 가장 쉽게 떠올릴 수 있는 방법은 각 문장들의 발생 빈도를 기반으로 하는 것입니다. 예를 들어, 우리에게 10000 문장의 대화 데이터가 주어졌는데, 그 중에서 200 문장이 ‘안녕’이라는 문장이라면 해당 문자열*string*의 발생 확률은 200/10000 = 2% 정도 될 것이라고 추정해 볼 수 있는 것이죠. 주어진 데이터에 기반하여 가능도*likelihood*가 최대가 되도록 확률을 추정하는 이러한 방법을 최대 가능도 추정법*MLE, Maximum Likelihood Estimation*이라고 부릅니다. 확률론에 기반하고 있는 추정 방법이지만 이 글에서는 깊게 다루지 않습니다. 단순히 ‘문자의 발생빈도/전체빈도’를 기준으로 각 문자열의 발생 확률을 추정하는 방법이라고만 이해하고 넘어가도록 하겠습니다.

빈도를 기반으로 확률을 추정할 수 있게 된 것은 좋은데, 문장 레벨에서 발생 빈도를 카운트 하다보면 발생하는 문제가 있습니다. 우선, 우리가 방금 살펴본 MLE는 각 문장의 발생 확률을 ‘정확한 법칙에 따라 계산’하는 것이 아니라 ‘데이터에 기반하여 추정’하는 방법이라는 사실을 기억할 필요가 있습니다. 또한 MLE에서는 기본적으로 주어진 데이터(샘플)가 전체 언어(모집단)을 대표하기에 충분하다는 것을 가정합니다. 따라서 전체 언어를 대표할 수 있을만한 충분한 데이터를 수집하는 것은 무엇보다 중요한 일이 되는데, ‘언어의 변동성’은 이러한 데이터를 수집하는 것을 매우 어렵게 만듭니다. 다음 단락에서는 ‘언어의 변동성’이 무엇인지 살펴보도록 하겠습니다.

## 언어의 변동성 *A Possible Variability of Languages*
언어가 가지는 기본적인 성질들 중 하나인 ‘변동성’은, 언어의 변화 가능성이 무한하다는 것을 의미합니다. 즉, 동일한 의미를 지니는 하나의 문자열은 여러 형태를 지닐 수 있다는 의미입니다. 예를 들어, 앞서 살펴보았던 ‘오늘은 좀 쉬려고!’라는 문장은, 아래와 같이 동일한 의미를 가지는 수없이 많은 문장들로 고쳐쓸 수 있습니다.

> 1-1) 집에서 오늘은 좀 쉬려고!
> 1-2) 오늘은 집에서 조금 쉬려고!
> 1-3) 오늘 그냥 집에서 좀 쉬려고!
> 1-4) 오늘은 좀 쉴래!

문제는, 이러한 형태 변화가 무한히 가능하기 때문에, 우리가 언어 모델링을 위해 아무리 많은 데이터를 수집한다고 하더라도 다양한 형태들 중 일부만을 포함할 수 밖에 없다는 데에 있습니다. 다시 말해, 언어의 변화 가능성은 무한한 데에 반해 우리의 데이터는 한정적일 수 밖에 없으므로, 언어 모델링에 사용되는 데이터는 전체 언어를 충분히 대표하지 못하게 됩니다. 따라서, 우리가 수집한 데이터에는 나타나지 않은 문장들의 실제 발생 확률은 0이 아님에도 불구하고, MLE 기반의 확률 추정에서는 해당 문장들의 발생 확률을 모두 0으로 추정하는 문제가 발생합니다. 예를 들어, 우리가 수집한 데이터에 `(1) 오늘은 좀 쉬려고!`라는 문장이 있었더라도, 문장 레벨에서의 MLE에서는 동일한 의미를 지니고 있는 `1-1`, `1-2`, `1-3`, `1-4` 문장의 발생 확률을 0으로 추정할 수도 있습니다. 그리고 이러한 문제는 문장 레벨에서 발생 빈도를 카운트할 때 더욱 두드러지게 되는데, 정확하게 동일한 문장이 관측될 확률은 동일한 단어가 관측될 확률에 비해 상대적으로 작을 수 밖에 없기 때문입니다. 

또 다른 예시로 ‘사랑하다’라는 의미의 ‘love’라는 영어 단어를 한번 살펴볼까요? 해당 단어는 주어의 단/복수 여부에 따라, 시제에 따라, 품사 변화에 따라 loves, loved, loving 등 매우 다양한 형태의 형태로 변화할 수 있습니다. 이러한 단어의 형태 변화를 굴절/파생 형변환*inflectional/derivational morphology*이라고 부르며, 언어의 무한한 변동성의 하나의 원인이 됩니다. 단어 레벨에서의 변동성을 완화하기 위한 방법으로는 어간 추출*stemming*과 표제어 추출*lemmatization*과 같은 방법들을 사용할 수 있습니다.

정리해보겠습니다. 언어는 단어 레벨에서의 형 변환과 문장 레벨에서의 단어 조합으로 인해 무수히 많은 변화 가능성을 지니고 있습니다. 이러한 언어의 변동성은 우리가 수집한 데이터가 전체 언어를 정확하게 대표하는 것을 불가능하게 만들고, 이는 MLE를 통해 정확한 확률을 추정하는 것을 어렵게 만듭니다. 단어 레벨에서의 형 변환은 어간 추출이나 표제어 추출과 같은 방법을 통해 완화될 수 있으나, 이 글에서는 자세히 다루지 않습니다. 문장 레벨에서의 단어 조합으로 인한 변동성은 문장을 단어들의 조합으로 다룸으로써 완화될 수 있으며, 다음 단락에서 자세히 살펴보도록 하겠습니다.

## 문장 = 단어들의 조합
제한된 데이터로 문장의 발생 확률을 예측하려고 하다보니, 우리가 수집한 데이터에 있는 문장들과 조금이라도 다른 문장들의 발생 확률들은 모두 0으로 추정하게 되었습니다. 우리는 ‘문장*sentence*’을 조금 작은 단위인 ‘단어들의 조합*combination of words*’라는 관점에서 바라봄으로써 이러한 문제를 다소 완화할 수 있습니다. 즉, n개의 단어로 구성된 문장 $Sentence\_n$은 각 단어 $word\_n$을 통해 다음과 같이 표현될 수 있습니다. 아래에서 특수문자 ‘;’는 이어붙임*concatenation*을 나타냅니다.

$$ sentence\_n = word\_1; word\_2; word\_3; \ldots; word\_n $$

이러한 관점 전환을 통해, 우리는 이제 문장의 발생 확률을 각 등장 단어들의 발생 확률의 곱셈을 통해 추정할 수 있게 되었습니다. 따라서, 이제는 우리가 확률을 추정하고자 하는 문자열과 정확하게 동일한 문자열이 수집한 데이터에 포함되어 있지 않더라도 0이 아닌 확률을 추정할 수 있습니다.

- $ P(오늘은\ 집에서\ 좀\ 쉬려고 \!) = P(오늘은) \times P(집에서) \times P(좀) \times P(쉬려고) \times P(!) $
- $ P(집에서\ 오늘은\ 좀\ 쉬려고 \!) = P(집에서) \times P(오늘은) \times P(좀) \times P(쉬려고) \times P(!) $

그러나 이러한 접근법에 장점만 있는 것은 아닙니다. 문장을 단어들의 조합으로 분할하는 과정에서, 단어가 등장하는 순서를 고려할 수 없게 되기 때문입니다. 다음의 두 문장의 예시를 통해 살펴보도록 하겠습니다.
> 5-1) I am traveling from Seoul to Edinburgh.
> 5-2) I am traveling from Edinburgh to Seoul.

각 문장에서 사용되고 있는 단어는 정확하게 동일하기 때문에, 단어 레벨에서의 MLE를 이용하면 두 문장의 발생 확률은 동일하게 추정됩니다. 문제는, From Seoul과 from Edinburgh, 혹은 to Edinburgh와 to Seoul은 다른 의미를 가지고 있고 발생 확률도 다를텐데, 현재 우리의 언어 모형은 이러한 단어 순서에 따른 차이를 전혀 고려하지 못한다는 점입니다. 

지금까지의 내용을 정리하자면, 우리는 MLE를 통해 문자열의 확률을 추정하고 있습니다. 그리고 데이터에서 관측되지 않은 문자열에 대한 발생 확률을 모두 0으로 추정하는 것은 MLE의 한계라고 볼 수 있습니다. 우리는 문장 레벨이 아닌 단어 레벨에서 빈도를 카운트 함으로써 이를 일정 수준 완화할 수 있으나, 더 이상 단어의 발생 순서를 고려할 수 없게된다는 새로운 문제와 마주하게 됩니다. 다음 글에서는 이러한 문제를 완화하기 위한 방법인 n-grams에 대해 살펴보도록 하겠습니다.

## 요약
- 언어 모델링은 각 후보 문자열에 확률을 할당하는 과정이며, 언어 모델은 각 문자열의 확률 분포를 나타낸다.
- 문자열의 발생 확률은 MLE를 통해 추정할 수 있다.
- 문장 레벨에서의 문자열은 발생 빈도가 너무 낮기 때문에, 단어 레벨로 분할하여 확률을 추정할 수 있다.
- 문장을 단어 레벨로 분할할 경우 단어의 순서를 고려할 수 없다.
